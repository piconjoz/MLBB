{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou58DdcYK1Ot"
      },
      "source": [
        "# AirBnB Dataset Prediction\n",
        "\n",
        "Testing linear regression (and 1 other algorithm) for predicting price of an AirBnB listing based on various features such as amenities, location and other factors.\n",
        "\n",
        "The dataset used is from https://insideairbnb.com/get-the-data/. Credits to author.\n",
        "\n",
        "1 main city will be chosen for analysis, **London, UK**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si8BWmhAgeOT"
      },
      "source": [
        "**What the Dataset Columns Entail**\n",
        "\n",
        "https://docs.google.com/spreadsheets/d/1iWCNJcSutYqpULSQHlNyGInUvHg2BoUGoNRIGa6Szc4/edit?gid=1322284596#gid=1322284596"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_VRIwgTLyNm"
      },
      "outputs": [],
      "source": [
        "# AIRBNB PRICE PREDICTION ANALYSIS\n",
        "import os\n",
        "\n",
        "cwd = os.getcwd()\n",
        "print(cwd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFAtGbLzNApX"
      },
      "outputs": [],
      "source": [
        "# move the Airbnb Dataset .gzip file to /content after downloading it at the website\n",
        "import gzip\n",
        "import shutil\n",
        "\n",
        "# download the gzip file from the link above\n",
        "\n",
        "compressed_name = 'london.csv.gz' # what ever name you choose for the gzip file\n",
        "file_name = 'listings.csv'\n",
        "\n",
        "with gzip.open(compressed_name, 'rb') as f_in:\n",
        "    with open(file_name, 'wb') as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji7mCdT-MA0T"
      },
      "source": [
        "The name of the file will be *listings.csv*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xIdCroVNljT"
      },
      "source": [
        "# 1. Data Cleaning and Preprocessing\n",
        "Clean the data and analyse the features for the most relevant data to be fed to the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_sJJhtaMIdU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# show the three rows\n",
        "# df = pd.read_csv('/content/listings.csv')\n",
        "df = pd.read_csv('listings.csv')\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2t9mlR1MPWPo"
      },
      "outputs": [],
      "source": [
        "# View the columns to see what's available in the dataset\n",
        "print(df.columns)\n",
        "print(len(df.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIPR_BWFPgIK"
      },
      "outputs": [],
      "source": [
        "# view what data types exists in the dataset\n",
        "# brief overview (uncomment to see)\n",
        "# df.dtypes\n",
        "\n",
        "# detailed overview\n",
        "df.info(verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCdd3xqVPp7c"
      },
      "outputs": [],
      "source": [
        "# How many rows are there before cleaning\n",
        "df.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcpjdIAqQG2N"
      },
      "outputs": [],
      "source": [
        "# Determine if there are null values within the dataset\n",
        "# isnull.().sum(axis=0) does not show everything\n",
        "# df.isnull().sum(axis=0)\n",
        "\n",
        "# rather than check everything, just view what columns have null values\n",
        "null_counts = df.isnull().sum()\n",
        "null_counts = null_counts[null_counts > 0]  # Filter only columns with nulls\n",
        "print(null_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRxFFuVWRcFd"
      },
      "source": [
        "We need to drop any unnecessary information that is not relevant to price calculation.\n",
        "\n",
        "We can start first with any irrelevant data that has no impact on price calculation whatsoever."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxz9IFpTS3SC"
      },
      "source": [
        "We want to ensure these columns remain :\n",
        "\n",
        "**Property & Listing Attributes**\n",
        "\n",
        "* property_type (Different types have different pricing, e.g., \"Apartment\" vs. \"Villa\")\n",
        "* room_type (Entire home/apt vs. private room affects price)\n",
        "* accommodates (More guests usually means higher price)\n",
        "* bathrooms_text (Number of bathrooms affects price)\n",
        "* bedrooms (More bedrooms typically increase price)\n",
        "* beds (More beds can increase listing value)\n",
        "* amenities (Luxury amenities like pools, WiFi, and AC may increase price)\n",
        "\n",
        "\n",
        "**Location**\n",
        "* latitude / longitude (Can be used to extract geo-based price trends)\n",
        "\n",
        "**Host Information** (If Significant)\n",
        "* host_is_superhost (Superhosts may charge higher prices)\n",
        "* host_listings_count / host_total_listings_count (Professional hosts vs. casual hosts may price differently)\n",
        "* host_identity_verified (Could be a trust factor affecting pricing)\n",
        "Availability & Minimum Nights\n",
        "* minimum_nights (Longer stays could impact price)\n",
        "* availability_30, availability_60, availability_90, availability_365 (More * * availability may suggest demand or pricing trends)\n",
        "\n",
        "**Reviews & Ratings**\n",
        "* number_of_reviews (More reviews may indicate demand)\n",
        "* review_scores_rating (Higher ratings may allow higher prices)\n",
        "* reviews_per_month (Shows frequency of bookings, indicating demand)\n",
        "\n",
        "Edit as necessary*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-z2yzUxXQZ53"
      },
      "outputs": [],
      "source": [
        "# List of columns to drop\n",
        "drop_columns = [\n",
        "    'id', 'listing_url', 'scrape_id', 'last_scraped', 'source',\n",
        "    'host_url', 'host_name', 'host_since', 'host_location', 'host_about',\n",
        "    'host_thumbnail_url', 'host_picture_url', 'host_verifications', 'host_neighbourhood',\n",
        "    'host_listings_count', 'host_total_listings_count', 'host_has_profile_pic',\n",
        "    'calendar_updated', 'calendar_last_scraped', 'first_review', 'last_review',\n",
        "    'neighbourhood', 'neighborhood_overview', 'neighbourhood_group_cleansed', 'license', 'picture_url', 'host_id',\n",
        "    'maximum_nights_avg_ntm', 'has_availability', 'calendar_last_scraped',\n",
        "    'number_of_reviews_ltm', 'number_of_reviews_l30d', 'first_review', 'last_review',\n",
        "    'license', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes',\n",
        "    'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms'\n",
        "]\n",
        "\n",
        "# double check this\n",
        "# columns to consider scrutizing\n",
        "# superhost? description? (no desc, less lightly to be bought)\n",
        "\n",
        "# Drop columns\n",
        "df_cleaned = df.drop(columns=drop_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oaiugujd__p"
      },
      "outputs": [],
      "source": [
        "# this is the new cleaned dataset\n",
        "df_cleaned.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LU9UqRseeO4"
      },
      "outputs": [],
      "source": [
        "# check for remaining missing values in the columns\n",
        "df_cleaned.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd7daKrVw08W"
      },
      "outputs": [],
      "source": [
        "# Drop the rows with empty fields\n",
        "# some empty is fine, but need to consider the following : price, bedrooms, bathroom\n",
        "df_cleaned = df_cleaned.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46lOa4kq8O_O"
      },
      "outputs": [],
      "source": [
        "# check how much we are left with and ensure that there are no more field empty within the dataset\n",
        "print(df_cleaned.isnull().sum())\n",
        "print(df_cleaned.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6e2wbYLsLlW"
      },
      "outputs": [],
      "source": [
        "# view how many unique object exists in each column\n",
        "include = ['object', 'float', 'int']\n",
        "df_cleaned.describe(include=include)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sElAaXOcmUj-"
      },
      "source": [
        "Since we are predicting the price, we have to convert the data type of the price column to a readable one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSon9vzNmbpe"
      },
      "outputs": [],
      "source": [
        "# parse the price column into readable float\n",
        "df_cleaned['price'] = df_cleaned['price'].str.replace('$', '').str.replace(',', '').astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQJp9NofmjU2"
      },
      "outputs": [],
      "source": [
        "df_cleaned.price.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQmZu2Bkxkrk"
      },
      "source": [
        "**Feature Cleaning**\n",
        "\n",
        "As a huge portion of the data is categorical in nature, we can use one hot encoding (https://www.kaggle.com/code/dansbecker/using-categorical-data-with-one-hot-encoding) to cateorize if an entry has *x* or *y* property or none at all.\n",
        "\n",
        "The next sections will be dealing with processing with columns that are categorical so as to allow the model to better understand the relationship of each feature and how it correlates to the prediction of the price.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Price analysis\n",
        "\n",
        "Before analysing the other features, we can start with the actual prices available first.\n",
        "\n",
        "We can plot out the range of prices available, the mean of price of all legitimate listings, the highest and lowest among other comparisons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Number of listing\") # Double checking\n",
        "print(df_cleaned.price.count())\n",
        "print(\"Mean price of all listings:\")\n",
        "print(df_cleaned.price.mean())\n",
        "print(\"Highest price of all listings:\")\n",
        "print(df_cleaned.price.max())\n",
        "print(\"Lowest price of all listings:\")\n",
        "print(df_cleaned.price.min())\n",
        "print(\"Median price of all listings:\")\n",
        "print(df_cleaned.price.median())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There might be a necessity to remove outliers. Feel free to discuss and determine if required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot the price range in histogram for seeing price distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_cleaned['price'], bins=140, kde=True)\n",
        "plt.xlabel(\"Price (Whichever Currency being used)\")\n",
        "plt.ylabel(\"Count of Listings\")\n",
        "plt.title(\"Distribution of Listing Prices\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Due to massive outliers in the price, its hard to see the actual distribution in price, we have to use other methods to see the proper distribution.\n",
        "\n",
        "It might be good practice to remove the high percentile listings as their values might be skewing the data heavily."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Removing high percentile outliers\n",
        "# Define a reasonable threshold (e.g., 99th percentile)\n",
        "price_threshold = np.percentile(df_cleaned[\"price\"], 95)\n",
        "\n",
        "# Filter data\n",
        "# Comment/Uncomment out the df_filtered line if you want to separate 2 datasets\n",
        "# 1 with all the listings, 1 with the top 5% percentile listings based on price removed.\n",
        "# df_filtered = df_cleaned[df_cleaned[\"price\"] <= price_threshold] # Comment/Uncomment accordingly\n",
        "df_cleaned = df_cleaned[df_cleaned[\"price\"] <= price_threshold]\n",
        "print(df_cleaned.shape[0])\n",
        "# print(df_filtered.shape[0]) # Comment/Uncomment accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replot\n",
        "plt.figure(figsize=(10, 6))\n",
        "# sns.histplot(df_filtered[\"price\"], bins=100, kde=True) # Comment/Uncomment accordingy\n",
        "sns.histplot(df_cleaned[\"price\"], bins=100, kde=True) # Uncomment this to run the filtered base dataset, if the the outlier was removed\n",
        "plt.xlabel(\"Price\")\n",
        "plt.ylabel(\"Count of Listings\")\n",
        "plt.title(\"Distribution of Listing Prices (Outliers Removed)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Log transformation post filtered\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(np.log1p(df_cleaned[\"price\"]), bins=100, kde=True)  # log1p avoids log(0)\n",
        "plt.xlabel(\"Log(Price + 1)\")\n",
        "plt.ylabel(\"Count of Listings\")\n",
        "plt.title(\"Log-Transformed Distribution of Listing Prices\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Boxplot to see outliers\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.boxplot(x=df_cleaned[\"price\"])\n",
        "plt.xlabel(\"Price\")\n",
        "plt.title(\"Boxplot of Listing Prices\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can the majority of the distribution lies in the area between 100~200 range. \n",
        "\n",
        "If the outlier was not removed, then the outlier at the far right should be around 400+ range. We can adjust accordingly as we proceeed to determine if we need the outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For fair analysis, it might be good to remove the outliers, as it may skew the numbers greatly. We can do so at the end processing and data engineering for easier splitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SZdQtfDgUbJ"
      },
      "source": [
        "## 1.2 Property & Physical Attributes\n",
        "\n",
        "The next few sections will be dedicated for processing the features of the physical atributes of the AirBnB listings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InVx2T07c8Vg"
      },
      "source": [
        "### 1.2.1 Cleaning of Physical Attributes Features\n",
        "\n",
        "Each features must be scrutinized and cleaned to be fitted into the model. Amenities, room types, bathroom and bedrooms and then price."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8h31s9oGGK4"
      },
      "source": [
        "**Amenities**\n",
        "\n",
        "We will first start of with amenities as an example.\n",
        "\n",
        "We need to one-hot encoding for all the amenities listed within the given amenity column.\n",
        "\n",
        "The format of the amenities is listed as an array i.e.\n",
        "\n",
        "[ \"Hangers\", \"Wi-Fi\", ... \"etc\" ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UGJ8lCvuexo"
      },
      "outputs": [],
      "source": [
        "# need to start fitting the data for a correlation map\n",
        "# get the unique values for all the amenities\n",
        "# and encode them for fitting\n",
        "import ast\n",
        "\n",
        "df_cleaned['amenities'] = df_cleaned['amenities'].apply(ast.literal_eval)\n",
        "\n",
        "# Get all unique amenities\n",
        "unique_amenities = set()\n",
        "df_cleaned['amenities'].apply(lambda x: unique_amenities.update(x))  # Flatten all amenities\n",
        "\n",
        "# Convert to sorted list for readability\n",
        "unique_amenities = sorted(unique_amenities)\n",
        "\n",
        "# Display all unique amenities\n",
        "print(unique_amenities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx6uFRQTxqhJ"
      },
      "source": [
        "The sheer number of unique amenity values means that one-hot encoding will result in too much columns for amenities, resulting in overfitting.\n",
        "\n",
        "To fix this, we need to categorise the amenities into broad categories to ensure reliable prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edGFzoKBFU1R"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define categorized amenities\n",
        "wifi_amenities = {\"Wifi\", \"Wi-Fi\", \"Ethernet connection\", \"無線lan\", \"ワイヤレスインターネット\"}\n",
        "kitchen_amenities = {\"Kitchen\", \"Microwave\", \"Refrigerator\", \"Stove\", \"Oven\", \"BBQ grill\", \"Coffee maker\", \"Dining table\"}\n",
        "parking_amenities = {\"Parking\", \"Garage\", \"EV charger\", \"free parking\", \"paid parking\", \"carport\"}\n",
        "pool_amenities = {\"Pool\", \"Swimming pool\", \"Hot tub\"}\n",
        "air_conditioning_amenities = {\"Air conditioning\", \"Heating\", \"Ceiling fan\", \"Indoor Fireplace\"}\n",
        "security_amenities = {\"Smoke alarm\", \"Fire extinguisher\", \"Carbon monoxide alarm\"}\n",
        "laundry_amenities = {\"Washer\", \"Dryer\", \"Iron\", \"Clothing storage\", \"Housekeeping\"}\n",
        "bathroom_amenities = {\"Bathtub\", \"Bidet\", \"Hot water\", \"Body Soap\", \"Shampoo\", \"Conditioner\"}\n",
        "entertainment_amenities = {\"TV\", \"HDTV\", \"Sound System\", \"Game console\", \"Streaming services\"}\n",
        "fitness_amenities = {\"Exercise equipment\", \"Gym\"}\n",
        "child_friendly_amenities = {\"Baby Monitor\", \"Baby bath\", \"High chair\", \"Crib\", \"Children’s books and toys\"}\n",
        "outdoor_amenities = {\"Backyard\", \"BBQ grill\"}\n",
        "elevator_amenities = {\"Elevator\"}\n",
        "\n",
        "# Combine all categories into one dictionary\n",
        "all_amenities = {\n",
        "    \"has_wifi\": wifi_amenities,\n",
        "    \"has_kitchen\": kitchen_amenities,\n",
        "    \"has_parking\": parking_amenities,\n",
        "    \"has_pool\": pool_amenities,\n",
        "    \"has_air_conditioning\": air_conditioning_amenities,\n",
        "    \"has_security\": security_amenities,\n",
        "    \"has_laundry\": laundry_amenities,\n",
        "    \"has_bathroom\": bathroom_amenities,\n",
        "    \"has_entertainment\": entertainment_amenities,\n",
        "    \"has_fitness\": fitness_amenities,\n",
        "    \"has_child_friendly\": child_friendly_amenities,\n",
        "    \"has_outdoor\": outdoor_amenities,\n",
        "    \"has_elevator\": elevator_amenities,\n",
        "}\n",
        "\n",
        "# Convert amenity strings into lists\n",
        "df_cleaned['amenities'] = df_cleaned['amenities'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "# Apply category checks: True if any of the category amenities are in the listing\n",
        "for column, category_set in all_amenities.items():\n",
        "    df_cleaned[column] = df_cleaned['amenities'].apply(lambda x: any(item in category_set for item in x) if isinstance(x, list) else False)\n",
        "\n",
        "# Also create columns for each **individual** amenity\n",
        "for category_set in all_amenities.values():\n",
        "    for amenity in category_set:\n",
        "        col_name = f\"has_{amenity.lower().replace(' ', '_')}\"  # Normalize column name\n",
        "        df_cleaned[col_name] = df_cleaned['amenities'].apply(lambda x: amenity in x if isinstance(x, list) else False)\n",
        "\n",
        "print(df_cleaned.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhikuWbfudfP"
      },
      "source": [
        "Some of the amenity classification has 0 listings, rendering it moot, so just drop them.\n",
        "\n",
        "Should drop them if all listings have said amenity, as it would be meaningless to classify."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvwOVBPuuHDu"
      },
      "outputs": [],
      "source": [
        "# check how many listings has\n",
        "for amenity in df_cleaned.columns:\n",
        "    if amenity.startswith('has_'):\n",
        "        print(f\"{amenity}: {df_cleaned[amenity].sum()} listings have this\")\n",
        "        if (df_cleaned[amenity].sum() == 0 or df_cleaned[amenity].sum() == df_cleaned.shape[0]):\n",
        "            df_cleaned = df_cleaned.drop(columns=[amenity])\n",
        "            print(f\"Dropping {amenity}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwJ3x0h4hSp0"
      },
      "outputs": [],
      "source": [
        "# Final amenity list\n",
        "df_cleaned.columns.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwR3wh5R3nOV"
      },
      "source": [
        "Now we have the other unique values to process. This includes :\n",
        "\n",
        "* property_type\n",
        "* room_type\n",
        "* bathrooms\n",
        "* bathrooms_type\n",
        "* bedrooms\n",
        "* bedrooms_type\n",
        "* accomodates\n",
        "\n",
        "Other variables can be accounted for being used as factors, the aforementioned should be prioritised\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irX8FRhgz8a2"
      },
      "source": [
        "**Room Type / Property Type**\n",
        "\n",
        "Next will be the *room_type* and *property_type* of each listing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnO1shJJ7pY3"
      },
      "outputs": [],
      "source": [
        "# Each value listed is shown to be either shared or private.\n",
        "df_cleaned['property_type'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a23amunxb1E"
      },
      "outputs": [],
      "source": [
        "# This column is more consistent in its categorisation\n",
        "df_cleaned['room_type'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wOUqBxRVwa6"
      },
      "source": [
        "Between the 2, it better to use room_type as a better baseline comparison to avoid complicating these features.\n",
        "\n",
        "We can drop *property_type* as a result, and just one-hot encode *room_type*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQ4ICwwyWry3"
      },
      "outputs": [],
      "source": [
        "# get rid of property type\n",
        "df_cleaned = df_cleaned.drop(columns=['property_type'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dnN7PGLYNsX"
      },
      "outputs": [],
      "source": [
        "# and one-hot encode the room types\n",
        "df_cleaned = pd.get_dummies(df_cleaned, columns=['room_type'], prefix=['room_type'])  # One-hot encode room_type\n",
        "df_cleaned.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xk7QQRh7Y8E2"
      },
      "outputs": [],
      "source": [
        "df_cleaned.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts5OktViZOdA"
      },
      "source": [
        "**Bathrooms / Bathrooms Text**\n",
        "\n",
        "Next will be the bathroom features *bathrooms* and *bathroom_text*. Similar to room and property features, we are going to focus on the *bathrooms* value only as it is already standardized.\n",
        "\n",
        "The unique values will still be shown in case anyone wants to make use of the unique values in *bathroom_text*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ5R4culbeSK"
      },
      "outputs": [],
      "source": [
        "print(\"Unique numbers of bathrooms:\\n\")\n",
        "print(df_cleaned.bathrooms.unique())\n",
        "print(df_cleaned.bathrooms.value_counts())\n",
        "print(\"\\nUnique bathroom types in bathroom_text:\\n\")\n",
        "print(df_cleaned.bathrooms_text.unique())\n",
        "print(df_cleaned.bathrooms_text.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7jzbwpOb_5Z"
      },
      "outputs": [],
      "source": [
        "# drop bathroom_text for now, only normalise bathrooms if need to\n",
        "df_cleaned = df_cleaned.drop(columns=['bathrooms_text'])\n",
        "df_cleaned.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There's a listing with 16 baths listed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bath_16 = df_cleaned.loc[df_cleaned['bathrooms'] > 10]\n",
        "print(bath_16['price'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This might skew the pricing, but we can leave it in first; We have no real way to tell if bathrooms really have a strong correlation with price."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI56zIWLdwVX"
      },
      "source": [
        "**Bedrooms / Beds**\n",
        "\n",
        "Any AirBnB should have beds and bedrooms, but apparently there might be some with 0 beds and bedrooms.\n",
        "\n",
        "There's no need to clean these values at the moment, so we can leave them be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e86eI1waeE6r"
      },
      "outputs": [],
      "source": [
        "print(\"Unique count of bedrooms:\\n\")\n",
        "print(df_cleaned.bedrooms.unique())\n",
        "print(df_cleaned.bedrooms.value_counts())\n",
        "print(\"\\nUnique count of beds:\\n\")\n",
        "print(df_cleaned.beds.unique())\n",
        "print(df_cleaned.beds.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fyhN2nycjI6"
      },
      "source": [
        "Aparently, there are listings with more than 15+ bedrooms.\n",
        "\n",
        "For beds, there are similar listings with beds over 15+ to 21?\n",
        "\n",
        "Realistically this outliers should be deleted, but feel free to keep it for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXL1LcZEdwLK"
      },
      "outputs": [],
      "source": [
        "# delete the rows with unrealistic beds and bedrooms\n",
        "df_cleaned = df_cleaned[df_cleaned['bedrooms'] <= 10]\n",
        "df_cleaned = df_cleaned[df_cleaned['beds'] <= 10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m33ImxJ3jwxv"
      },
      "source": [
        "**Accomodates**\n",
        "\n",
        "How many guests can a listing accomodate in a booking. Presumably the greater the number the greater the price.\n",
        "\n",
        "Similarly there's no need to clean it, since its a whole number with no set limit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df_cleaned.accommodates.unique())\n",
        "print(df_cleaned.accommodates.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TykFKuBag185"
      },
      "source": [
        "### 1.2.2 Analysis of Physical Attributes\n",
        "\n",
        "This section we will be delving deeper into analysing each feature and its relationship to a listing's price.\n",
        "\n",
        "First, we can get a heatmap to see just how much of these features correlate to the price of each listing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlfYOB9H533R"
      },
      "source": [
        "**Amenities analysis**\n",
        "\n",
        "Keep in mind that correlation heatmaps are not entirely accurate to show the true relationship of feature to a listing's price. Amenities for example only have true/false values and not much variance; other forms of analysis may be better suited to understand which feature may have stronger weightage on price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w73Hb3E-i2bY"
      },
      "outputs": [],
      "source": [
        "# amenities only heatmap\n",
        "amenities_attribute_corr_columns = ['price']\n",
        "amenities_attribute_corr_columns.extend([col for col in df_cleaned.columns if col.startswith('has_')])\n",
        "corr_matrix = df_cleaned[amenities_attribute_corr_columns]\n",
        "# get rid of any columns that have constant values, offers nothing to correlate\n",
        "corr_matrix = corr_matrix.loc[:, corr_matrix.nunique() > 1]\n",
        "corr_matrix = corr_matrix.corr()\n",
        "\n",
        "plt.figure(figsize=(30,30))\n",
        "# sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5, annot_kws={\"size\": 8})\n",
        "\n",
        "plt.title(\"Amenity Feature Correlation Heatmap\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bClztB4Dog3R"
      },
      "outputs": [],
      "source": [
        "# Top 20 Most Correlated Features to 'Price'\n",
        "target_feature = 'price'\n",
        "top_10_corr = corr_matrix[target_feature].abs().sort_values(ascending=False)[1:21]  # Exclude self-correlation\n",
        "\n",
        "# Plot Top 20 Correlations\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top_10_corr.values, y=top_10_corr.index, palette=\"coolwarm_r\", hue=top_10_corr.index, legend=\"auto\")  # \"_r\" reverses colors\n",
        "plt.xlabel(\"Correlation\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(f\"Top 20 Features Correlated with {target_feature}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx1uHf0KW7K5"
      },
      "source": [
        "Apparently having a dinner table in your AirBnB might have the greatest positive impact on your price listing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xKhJFfDWxfd"
      },
      "source": [
        "We can do other forms of analysis with amenities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3GjDM2zW5Ll"
      },
      "outputs": [],
      "source": [
        "# Current mean of all listings without any changes\n",
        "print(\"Current price mean:\")\n",
        "print(df_cleaned.price.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyX9cgVjkhf8"
      },
      "source": [
        "We can check the mean price of all listings with any one amenitiy, and check against the mean price of all listings.\n",
        "\n",
        "By doing so, we can see if there are any sizable increase in mean price listings based if a listing has a particular amenity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ-WDH6pYQrR"
      },
      "outputs": [],
      "source": [
        "amenities_attribute_corr_columns = ['price']\n",
        "amenities_attribute_corr_columns.extend([col for col in df_cleaned.columns if col.startswith('has_')])\n",
        "check_mean = df_cleaned[amenities_attribute_corr_columns]\n",
        "\n",
        "# Create a results list\n",
        "results = []\n",
        "\n",
        "# Iterate through each amenity column (excluding 'price')\n",
        "for amenity in amenities_attribute_corr_columns[1:]:\n",
        "    mean_with = check_mean.loc[check_mean[amenity] == 1, \"price\"].mean()\n",
        "    mean_without = check_mean.loc[check_mean[amenity] == 0, \"price\"].mean()\n",
        "\n",
        "    # Append to results\n",
        "    results.append([amenity, mean_with, mean_without])\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_amenity_means = pd.DataFrame(results, columns=[\"Amenity\", \"Mean Price (With)\", \"Mean Price (Without)\"])\n",
        "\n",
        "# Display the DataFrame\n",
        "df_amenity_means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XLuY-A7loL0"
      },
      "source": [
        "We can list out the prices and compare them to see if they are above the mean or below it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWcgsYRLZ_0R"
      },
      "outputs": [],
      "source": [
        "# get rid of the columns with empty values\n",
        "df_amenity_means = df_amenity_means.dropna()\n",
        "\n",
        "# Sort by price difference\n",
        "df_amenity_means[\"Price Difference\"] = df_amenity_means[\"Mean Price (With)\"] - df_amenity_means[\"Mean Price (Without)\"]\n",
        "df_amenity_means = df_amenity_means.sort_values(\"Price Difference\", ascending=False)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(18, 10))\n",
        "sns.barplot(data=df_amenity_means, x=\"Price Difference\", y=\"Amenity\", palette=\"coolwarm_r\", legend=\"auto\", hue=\"Amenity\")\n",
        "plt.title(\"Impact of Amenities on Listing Price\")\n",
        "plt.xlabel(\"Difference in Mean Price\")\n",
        "plt.ylabel(\"Amenity\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIQRSAeIliD3"
      },
      "source": [
        "Below is a chi-squared analysis of amenity and prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Av-e_-ypkTBx"
      },
      "outputs": [],
      "source": [
        "# Chi Squared Analysis\n",
        "import scipy.stats as stats\n",
        "\n",
        "# # Interpretation\n",
        "# if p < 0.05:\n",
        "#     print(\"Significant relationship between has_pool and price category!\")\n",
        "# else:\n",
        "#     print(\"No significant relationship found.\")\n",
        "# Select relevant columns\n",
        "amenities_attribute_corr_columns = ['price']\n",
        "amenities_attribute_corr_columns.extend([col for col in df_cleaned.columns if col.startswith('has_')])\n",
        "chi_sq = df_cleaned[amenities_attribute_corr_columns].copy()\n",
        "\n",
        "# Create price categories\n",
        "chi_sq['price_category'] = pd.qcut(chi_sq['price'], q=3, labels=[\"Low\", \"Medium\", \"High\"])\n",
        "print(chi_sq['price_category'].value_counts())\n",
        "\n",
        "# Store results\n",
        "significant_amenities = []\n",
        "\n",
        "# Iterate over all amenities\n",
        "for amenity in [col for col in chi_sq.columns if col.startswith('has_')]:\n",
        "    contigency_table = pd.crosstab(chi_sq['price_category'], chi_sq[amenity])\n",
        "\n",
        "    # Perform Chi-Square test\n",
        "    chi2, p, dof, expected = stats.chi2_contingency(contigency_table)\n",
        "\n",
        "    # Check significance (p < 0.05 means significant relationship)\n",
        "    if p < 0.05:\n",
        "        significant_amenities.append((amenity, p))\n",
        "\n",
        "# Sort by p-value (ascending)\n",
        "significant_amenities.sort(key=lambda x: x[1])\n",
        "\n",
        "print(f\"{len(significant_amenities)} amenities have postive impact on price.\\n\")\n",
        "\n",
        "# Print results\n",
        "print(\"Amenities with a significant relationship to price category:\")\n",
        "for amenity, p_value in significant_amenities:\n",
        "    print(f\"{amenity}: p-value = {p_value:.5f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL4m5JT7XNHn"
      },
      "source": [
        "**Room types**\n",
        "\n",
        "Analysing what kind of effect the room type has on price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKdXXL9yklIJ"
      },
      "outputs": [],
      "source": [
        "# room_types only heatmap\n",
        "room_types_attribute_corr_columns = ['price', 'room_type_Entire home/apt', 'room_type_Hotel room', 'room_type_Private room', 'room_type_Shared room']\n",
        "corr_matrix = df_cleaned[room_types_attribute_corr_columns].corr()\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "\n",
        "plt.title(\"Room Type Feature Correlation Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89IXZmURl1y3"
      },
      "source": [
        "**Other physical attributes**\n",
        "\n",
        "Here we can see the heatmap and other visual representation for each feature.\n",
        "\n",
        "The graphs itself is pretty self-explanatory and simply shows the relationship between the features and price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaYxAmg0jdSy"
      },
      "outputs": [],
      "source": [
        "# the rest of the physical attribute features\n",
        "other_atritbutes_corr_columns = ['price', 'bathrooms', 'bedrooms', 'beds', 'accommodates']\n",
        "corr_matrix = df_cleaned[other_atritbutes_corr_columns].corr()\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "\n",
        "# Show plot\n",
        "plt.title(\"Other Attributes Feature Correlation Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNZ4Xv9Pf0W8"
      },
      "outputs": [],
      "source": [
        "# Price against bathrooms graph\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.lineplot(data=df_cleaned, x=\"bathrooms\", y=\"price\", errorbar=None)\n",
        "plt.xlabel(\"Number of Bathrooms Present\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Price vs. Amount of Bathrooms\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8NDnMGDbYDi"
      },
      "outputs": [],
      "source": [
        "# Price against bedrooms graph\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.lineplot(data=df_cleaned, x=\"bedrooms\", y=\"price\", errorbar=None)\n",
        "plt.xlabel(\"Number of Bedrooms\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Price vs. Bedrooms\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvg2IDrJfYqr"
      },
      "outputs": [],
      "source": [
        "# Price against beds graph\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.lineplot(data=df_cleaned, x=\"beds\", y=\"price\", errorbar=None)\n",
        "plt.xlabel(\"Number of Beds\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Price vs. Beds\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ud4oqizRgRQz"
      },
      "outputs": [],
      "source": [
        "# Price against accomodation graph\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.lineplot(data=df_cleaned, x=\"accommodates\", y=\"price\", errorbar=None)\n",
        "plt.xlabel(\"Number of Guests Accommodated\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Price vs. Guest Capacity\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy4ar_kBmPt-"
      },
      "source": [
        "The graphs for accomodation seems the most stable, while the others follow a shaky trend. Accomodation also has the highest correlation score, so this tracks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bptG7MJMpgYy"
      },
      "source": [
        "## 1.3 Location Attributes\n",
        "\n",
        "We can try to process the data with location as we are given the longitude and latitude for each listing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3.1 Cleaning and Processing of Location Attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "location_attributes=['price','longitude','latitude','neighbourhood_cleansed']\n",
        "df_cleaned[location_attributes].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYav0Ac0UL8w"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=df_cleaned, x=\"longitude\", y=\"latitude\", hue=\"price\", palette=\"coolwarm\", alpha=0.7)\n",
        "plt.xlabel(\"Longitude\")\n",
        "plt.ylabel(\"Latitude\")\n",
        "plt.title(\"Price Distribution by Location\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get basic descriptive info\n",
        "print(df_cleaned['neighbourhood_cleansed'].describe())\n",
        "# print(df_cleaned['neighbourhood_cleansed'].unique)\n",
        "print(df_cleaned['neighbourhood_cleansed'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cluster map\n",
        "import folium\n",
        "from folium.plugins import MarkerCluster\n",
        "from IPython.display import display\n",
        "\n",
        "# Create a base map centered around London\n",
        "m = folium.Map(location=[df_cleaned[\"latitude\"].mean(), df_cleaned[\"longitude\"].mean()], zoom_start=11)\n",
        "\n",
        "# Create a marker cluster (to group nearby points)\n",
        "marker_cluster = MarkerCluster().add_to(m)\n",
        "\n",
        "# Add points to the map\n",
        "for _, row in df_cleaned.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row[\"latitude\"], row[\"longitude\"]],\n",
        "        radius=5,\n",
        "        color=\"blue\",\n",
        "        fill=True,\n",
        "        fill_color=\"blue\",\n",
        "        fill_opacity=0.6,\n",
        "        popup=f\"Price: {row['price']}\"\n",
        "    ).add_to(marker_cluster)\n",
        "\n",
        "# Show the map\n",
        "display(m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Heatmap for distribution of listings\n",
        "from folium.plugins import HeatMap\n",
        "\n",
        "# Initialize map centered at the average location\n",
        "m = folium.Map(location=[df_cleaned[\"latitude\"].mean(), df_cleaned[\"longitude\"].mean()], zoom_start=12)\n",
        "\n",
        "# Prepare data: (latitude, longitude, price)\n",
        "heat_data = list(zip(df_cleaned[\"latitude\"], df_cleaned[\"longitude\"], df_cleaned[\"price\"]))\n",
        "\n",
        "# Add heatmap\n",
        "HeatMap(heat_data, radius=12, blur=8, max_zoom=13).add_to(m)\n",
        "\n",
        "# Save and display\n",
        "m.save(\"price_heatmap.html\")\n",
        "display(m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Nearest Distance to Any Station**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Author: https://www.doogal.co.uk/london_stations\n",
        "\n",
        "We managed to find a dataset containing all rail stations in all of London,UK, not just the Greater London area.\n",
        "This allows us to map the stations and see if the listings are near any train station. If they are, then the closer distance might result in a better pricing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.spatial import cKDTree\n",
        "from geopy.distance import geodesic\n",
        "\n",
        "# Load train station data (CSV format)\n",
        "stations_df = pd.read_csv(\"london info datasets/London stations.csv\")\n",
        "\n",
        "# Extract station coordinates\n",
        "station_coords = stations_df[[\"Latitude\", \"Longitude\"]].values\n",
        "\n",
        "# Build KDTree for fast lookup\n",
        "tree = cKDTree(station_coords)\n",
        "\n",
        "# Function to find nearest station efficiently\n",
        "def get_nearest_station(row):\n",
        "    listing_point = (row[\"latitude\"], row[\"longitude\"])\n",
        "    dist, idx = tree.query(listing_point)  # Find nearest station index\n",
        "    nearest_station = stations_df.iloc[idx][\"Station\"]  # Get station name\n",
        "    nearest_distance = geodesic(listing_point, station_coords[idx]).km  # Distance in km\n",
        "    return pd.Series([nearest_distance, nearest_station])\n",
        "\n",
        "# Apply function efficiently\n",
        "df_cleaned[[\"distance_to_nearest_station\", \"nearest_station\"]] = df_cleaned.apply(get_nearest_station, axis=1)\n",
        "\n",
        "# Normalize the distance (Min-Max Scaling)\n",
        "min_dist = df_cleaned[\"distance_to_nearest_station\"].min()\n",
        "max_dist = df_cleaned[\"distance_to_nearest_station\"].max()\n",
        "\n",
        "df_cleaned[\"normalized_distance_to_station\"] = (df_cleaned[\"distance_to_nearest_station\"] - min_dist) / (max_dist - min_dist)\n",
        "\n",
        "print(\"Updated listings with normalized station distances saved.\")\n",
        "\n",
        "# Append to location attributes\n",
        "location_attributes.append('distance_to_nearest_station')\n",
        "location_attributes.append('nearest_station')\n",
        "location_attributes.append('normalized_distance_to_station')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Cluster Grouping**\n",
        "\n",
        "Form regional group based on location of listings.\n",
        "\n",
        "If there are more listings in a particular area, group them into a cluster group. There are 2 types, one will be geographical based, the other will be through K-means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Geographical Cluster\n",
        "# Compute mean latitude and longitude\n",
        "mean_lat = df_cleaned[\"latitude\"].mean()\n",
        "mean_lon = df_cleaned[\"longitude\"].mean()\n",
        "\n",
        "# Function to assign NSEW clusters\n",
        "def assign_nsew_cluster(row):\n",
        "    if row[\"latitude\"] >= mean_lat and row[\"longitude\"] >= mean_lon:\n",
        "        return \"NE\"\n",
        "    elif row[\"latitude\"] >= mean_lat and row[\"longitude\"] < mean_lon:\n",
        "        return \"NW\"\n",
        "    elif row[\"latitude\"] < mean_lat and row[\"longitude\"] >= mean_lon:\n",
        "        return \"SE\"\n",
        "    else:\n",
        "        return \"SW\"\n",
        "\n",
        "# Apply clustering\n",
        "df_cleaned[\"nsew_cluster\"] = df_cleaned.apply(assign_nsew_cluster, axis=1)\n",
        "\n",
        "# Check distribution\n",
        "print(df_cleaned[\"nsew_cluster\"].value_counts())\n",
        "location_attributes.append('nsew_cluster')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Geographical Clustering graph\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(\n",
        "    x=df_cleaned[\"longitude\"],\n",
        "    y=df_cleaned[\"latitude\"],\n",
        "    hue=df_cleaned[\"nsew_cluster\"],\n",
        "    palette={\"NE\": \"red\", \"NW\": \"blue\", \"SE\": \"green\", \"SW\": \"orange\"},\n",
        "    alpha=0.6\n",
        ")\n",
        "plt.axvline(mean_lon, color=\"black\", linestyle=\"--\")  # Vertical line for E/W split\n",
        "plt.axhline(mean_lat, color=\"black\", linestyle=\"--\")  # Horizontal line for N/S split\n",
        "plt.title(\"NSEW Clusters Based on Latitude & Longitude\")\n",
        "plt.xlabel(\"Longitude\")\n",
        "plt.ylabel(\"Latitude\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can print out the average price for each region"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group by NSEW quadrant and calculate the average price\n",
        "avg_price_per_quadrant = df_cleaned.groupby(\"nsew_cluster\")[\"price\"].mean()\n",
        "\n",
        "# Print results\n",
        "print(avg_price_per_quadrant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# K-means clustering\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wcss = []\n",
        "K_range = range(1, 15)  # Try k from 1 to 15\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(df_cleaned[[\"longitude\", \"latitude\"]])\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the elbow method\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(K_range, wcss, marker=\"o\")\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"WCSS (Inertia)\")\n",
        "plt.title(\"Elbow Method for Optimal k\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the graph, ~5 seems to be a good cutoff for the number of clusters, as it is where it starts stablising."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Define the number of clusters (tune this number)\n",
        "num_clusters = 5 # based of\n",
        "\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "df_cleaned[\"location_cluster\"] = kmeans.fit_predict(df_cleaned[[\"latitude\", \"longitude\"]])\n",
        "location_attributes.append('location_cluster')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(\n",
        "    x=df_cleaned[\"longitude\"],\n",
        "    y=df_cleaned[\"latitude\"],\n",
        "    hue=df_cleaned[\"location_cluster\"],\n",
        "    palette=\"viridis\",\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.title(\"Geographical Distribution of Clusters\")\n",
        "plt.xlabel(\"Longitude\")\n",
        "plt.ylabel(\"Latitude\")\n",
        "plt.legend(title=\"Cluster\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# also print out the average prices for the k-cluster\n",
        "# Group by NSEW quadrant and calculate the average price\n",
        "avg_price_per_cluster = df_cleaned.groupby(\"location_cluster\")[\"price\"].mean()\n",
        "\n",
        "# Print results\n",
        "print(avg_price_per_cluster)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Visual Analysis of Price Density and Proximity**\n",
        "\n",
        "Looking at the price density graph, it is apparent that listings near the middle have higher prices. This could be an indication that listings near the middle of London (i.e. in Central London) could have higher price due to proximity to central facilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Price Density heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.kdeplot(\n",
        "    x=df_cleaned[\"longitude\"],\n",
        "    y=df_cleaned[\"latitude\"],\n",
        "    weights=df_cleaned[\"price\"],\n",
        "    cmap=\"coolwarm\",\n",
        "    fill=True,\n",
        "    alpha=0.6\n",
        ")\n",
        "plt.title(\"Heatmap of Price Density\")\n",
        "plt.xlabel(\"Longitude\")\n",
        "plt.ylabel(\"Latitude\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Density heatmap of physical listings\n",
        "sns.kdeplot(\n",
        "    x=df_cleaned[\"longitude\"],\n",
        "    y=df_cleaned[\"latitude\"],\n",
        "    cmap=\"Blues\",\n",
        "    fill=True,\n",
        "    alpha=0.6\n",
        ")\n",
        "plt.title(\"Heatmap of Listing Density\")  # This graph is for density of listings (how many listings are concentrated)\n",
        "plt.xlabel(\"Longitude\")\n",
        "plt.ylabel(\"Latitude\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the densest point of the map (in this case using the gassian_kde estimate on the coordinates, based on the maps the middle point is roughly 51.50~,-0.2-1~) to roughly gauge where the center is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import gaussian_kde\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Extract coordinates\n",
        "longitudes = df_cleaned[\"longitude\"].values\n",
        "latitudes = df_cleaned[\"latitude\"].values\n",
        "\n",
        "# Compute KDE for density estimation\n",
        "kde = gaussian_kde(np.vstack([longitudes, latitudes]))\n",
        "\n",
        "# Find the highest density point\n",
        "density_values = kde(np.vstack([longitudes, latitudes]))\n",
        "max_density_idx = np.argmax(density_values)\n",
        "central_lat, central_lon = latitudes[max_density_idx], longitudes[max_density_idx]\n",
        "\n",
        "print(f\"Determined Central Point: ({central_lat}, {central_lon})\")\n",
        "\n",
        "# Function to calculate Haversine distance\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Earth's radius in km\n",
        "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
        "    return R * c  # Distance in km\n",
        "\n",
        "# Compute distance from KDE-determined central point\n",
        "df_cleaned[\"distance_to_density_center\"] = df_cleaned.apply(\n",
        "    lambda row: haversine(row[\"latitude\"], row[\"longitude\"], central_lat, central_lon),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Standardize the distance using Min-Max Scaling (0 to 1)\n",
        "scaler = MinMaxScaler()\n",
        "df_cleaned[\"proximity_to_density_center\"] = 1 - scaler.fit_transform(df_cleaned[[\"distance_to_density_center\"]])\n",
        "\n",
        "# Print some results\n",
        "df_cleaned[[\"latitude\", \"longitude\", \"distance_to_density_center\", \"proximity_to_density_center\"]].head()\n",
        "location_attributes.append(\"distance_to_density_center\")\n",
        "location_attributes.append(\"proximity_to_density_center\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Distance to center is the raw value in kilometers. Proximity is the scaled value where if the value is closer to 1, it is closer to the center."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3.2 Analysis of Location Attributes\n",
        "\n",
        "Further breakdown of the location attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cleaned[location_attributes].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-hot encode 'nsew_cluster'\n",
        "df_cleaned = pd.get_dummies(df_cleaned, columns=[\"nsew_cluster\"], prefix=\"nsew\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop these. Encoding them will become to complicated and might overfit\n",
        "location_attributes.remove('nsew_cluster')\n",
        "location_attributes.remove('neighbourhood_cleansed')\n",
        "location_attributes.remove('nearest_station')\n",
        "location_attributes.remove('longitude')\n",
        "location_attributes.remove('latitude')\n",
        "# add the one hot geo cluster\n",
        "location_attributes.append('nsew_NE')\n",
        "location_attributes.append('nsew_NW')\n",
        "location_attributes.append('nsew_SE')\n",
        "location_attributes.append('nsew_SW')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cleaned[location_attributes].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corr_matrix = df_cleaned[location_attributes].corr()\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "\n",
        "plt.title(\"Heatmap of all location attributes\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(\n",
        "    x=df_cleaned[\"distance_to_nearest_station\"],\n",
        "    y=df_cleaned[\"price\"],\n",
        "    alpha=0.5\n",
        ")\n",
        "plt.title(\"Price vs. Distance to Nearest Station\")\n",
        "plt.xlabel(\"Distance to Nearest Station (Meters)\")\n",
        "plt.ylabel(\"Price (£)\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Location cluster seems ineffective in drawing any conclusions, while the geographical one seems to have more effect than the cluster. The distance and proximity to distance seems to also have strong correlation to price, but for distance to nearest station the effect seems present but minimal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFsXrZDQpzAw"
      },
      "source": [
        "## 1.4 Host Atttributes\n",
        "\n",
        "Feature processing with features corresponding to host information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KyhPDG8yG3S"
      },
      "source": [
        "The list of Host attribute features consists of:\n",
        "\n",
        " * 'host_response_time'\n",
        " * 'host_response_rate'\n",
        " * 'host_acceptance_rate'\n",
        " * 'host_is_superhost'\n",
        " * 'host_identity_verified'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RH0beGJaymGJ"
      },
      "outputs": [],
      "source": [
        "host_attributes = ['price','host_response_time','host_response_rate','host_acceptance_rate','host_is_superhost','host_identity_verified']\n",
        "df_cleaned[host_attributes].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FO6jDUWvL-Z"
      },
      "source": [
        "**Cleaning Features**\n",
        "\n",
        "We have to clean the data once again and see the relationship between these features and price."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6DDPCJXNtda"
      },
      "source": [
        "**Response Time and Rate, Acceptance Rate**\n",
        "\n",
        "These features are unique and have to be normalised/tagged before they are able to be used for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW9ARacwKiFO"
      },
      "outputs": [],
      "source": [
        "# pull the values and see unique counts, whats available\n",
        "# start with response time and rate, with acceptance rate\n",
        "print(\"Response time data:\\n\")\n",
        "print(df_cleaned.host_response_time.unique())\n",
        "print(df_cleaned.host_response_time.value_counts())\n",
        "print(\"\\nResponse rate data:\\n\")\n",
        "print(df_cleaned.host_response_rate.unique())\n",
        "print(df_cleaned.host_response_rate.value_counts())\n",
        "print(\"\\nAcceptance rate data:\\n\")\n",
        "print(df_cleaned.host_acceptance_rate.unique())\n",
        "print(df_cleaned.host_acceptance_rate.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRO_HlHEQuEk"
      },
      "source": [
        "We can encode the values for the response time, and convert the percentages into numerical values and normalise them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elmSnx5mQ3pJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "unique_response_time = df_cleaned.host_response_time.unique()\n",
        "mapped = le.fit_transform(unique_response_time)\n",
        "\n",
        "mapping = dict(zip(mapped, unique_response_time))\n",
        "sorted_dict = dict(sorted(mapping.items()))\n",
        "print(sorted_dict)\n",
        "\n",
        "df_cleaned['host_response_time'] = le.fit_transform(df_cleaned['host_response_time'])\n",
        "# df_cleaned_no_outliers['host_response_time'] = le.fit_transform(df_cleaned_no_outliers['host_response_time'])\n",
        "df_cleaned['host_response_time'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnPz08XwSu4L"
      },
      "outputs": [],
      "source": [
        "# Convert percentage strings to float and normalize\n",
        "df_cleaned[\"host_response_rate\"] = df_cleaned[\"host_response_rate\"].str.rstrip(\"%\").astype(float) / 100\n",
        "df_cleaned[\"host_acceptance_rate\"] = df_cleaned[\"host_acceptance_rate\"].str.rstrip(\"%\").astype(float) / 100\n",
        "\n",
        "# Convert percentage strings to float and normalize\n",
        "# df_cleaned_no_outliers[\"host_response_rate\"] = df_cleaned_no_outliers[\"host_response_rate\"].str.rstrip(\"%\").astype(float) / 100\n",
        "# df_cleaned_no_outliers[\"host_acceptance_rate\"] = df_cleaned_no_outliers[\"host_acceptance_rate\"].str.rstrip(\"%\").astype(float) / 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoRLaFZzVM1T"
      },
      "outputs": [],
      "source": [
        "# show the new cleaned values\n",
        "df_cleaned[[\"host_response_rate\", \"host_acceptance_rate\"]].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0GB88sXLAdK"
      },
      "outputs": [],
      "source": [
        "# For superhost and verified columns\n",
        "print(df_cleaned.host_is_superhost.value_counts())\n",
        "print(f\"Percentage of hosts that are superhost: {(df_cleaned.host_is_superhost.value_counts().iloc[0]/df_cleaned.host_is_superhost.value_counts().sum())*100:.2f}\")\n",
        "print(df_cleaned.host_identity_verified.value_counts())\n",
        "print(f\"Percentage of hosts that are verified: {(df_cleaned.host_identity_verified.value_counts().iloc[0]/df_cleaned.host_identity_verified.value_counts().sum())*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HD7HgdyNrM2"
      },
      "outputs": [],
      "source": [
        "# one hot encode the superhost and verified columns\n",
        "df_cleaned[\"host_is_superhost\"] = df_cleaned[\"host_is_superhost\"].map({\"t\": True, \"f\": False})\n",
        "df_cleaned[\"host_identity_verified\"] = df_cleaned[\"host_identity_verified\"].map({\"t\": True, \"f\": False})\n",
        "# df_cleaned_no_outliers[\"host_is_superhost\"] = df_cleaned_no_outliers[\"host_is_superhost\"].map({\"t\": True, \"f\": False})\n",
        "# df_cleaned_no_outliers[\"host_identity_verified\"] = df_cleaned_no_outliers[\"host_identity_verified\"].map({\"t\": True, \"f\": False})\n",
        "\n",
        "df_cleaned[[\"host_is_superhost\", \"host_identity_verified\"]].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HttHBVy1ZbSf"
      },
      "source": [
        "**Analysis**\n",
        "\n",
        "Now we can further analyse the features and their relationship to price.\n",
        "\n",
        "As usual we can start with the heatmap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWh-85f5ZppY"
      },
      "outputs": [],
      "source": [
        "# generate heatmap\n",
        "corr_host = df_cleaned[host_attributes].corr()\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.heatmap(corr_host, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "\n",
        "# Show plot\n",
        "plt.title(\"Host Features Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2nx9OdFanKL"
      },
      "source": [
        "Lack of variance in data (i.e. actual numerical value) makes it hard to see positive relationships in heatmaps\n",
        "\n",
        "It would be better to see other forms of analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO7oYXVcbP2W"
      },
      "source": [
        "**Response Time**\n",
        "\n",
        "Deos response time seem to affect pricing? It seems to be somewhat affected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUFuEhKZaz35"
      },
      "outputs": [],
      "source": [
        "# Host response time vs. Price\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(x=\"host_response_time\", y=\"price\", data=df_cleaned)\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Price Distribution by Host Response Time\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDhJWwYJb6O0"
      },
      "source": [
        "Do higher response/acceptance rates lead to higher/lower prices?\n",
        "Are there clusters of hosts charging extreme prices with low engagement?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdDpz060bNZG"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(x=\"host_response_rate\", y=\"price\", data=df_cleaned, alpha=0.5)\n",
        "plt.title(\"Price vs. Host Response Rate\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oa5uPjT8cGc2"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(x=\"host_acceptance_rate\", y=\"price\", data=df_cleaned, alpha=0.5)\n",
        "plt.title(\"Price vs. Host Acceptance Rate\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8GOZEKtcE0z"
      },
      "outputs": [],
      "source": [
        "print(df_cleaned[\"price\"].mean())\n",
        "print(df_cleaned.groupby(\"host_response_time\")[\"price\"].mean().sort_values())\n",
        "print(df_cleaned.groupby(\"host_is_superhost\")[\"price\"].mean())\n",
        "\n",
        "# plot into bar graph\n",
        "# Calculate overall average price\n",
        "avg_price = df_cleaned[\"price\"].mean()\n",
        "\n",
        "# Create bar plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "ax = sns.barplot(x=\"host_response_time\", y=\"price\", data=df_cleaned, estimator=lambda x: x.mean())\n",
        "\n",
        "# Add a horizontal line for average price\n",
        "plt.axhline(avg_price, color='red', linestyle='dashed', linewidth=2, label=f'Avg Price: {avg_price:.2f}')\n",
        "\n",
        "# Annotate each bar with its mean price\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height():.2f}',\n",
        "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='bottom', fontsize=10, color='black')\n",
        "\n",
        "# Add title and legend\n",
        "plt.title(\"Average Price by Host Response Time\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3-4y9Qrb7d7"
      },
      "source": [
        "Are Superhosts charging more on average?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "On9YJxNReYq3"
      },
      "outputs": [],
      "source": [
        "# Host is superhost vs. Price\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.boxplot(x=\"host_is_superhost\", y=\"price\", data=df_cleaned)\n",
        "plt.title(\"Price Distribution for Superhosts vs. Non-Superhosts\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhhAleKEcjwZ"
      },
      "outputs": [],
      "source": [
        "# Create bar plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "ax = sns.barplot(x=\"host_is_superhost\", y=\"price\", data=df_cleaned, estimator=lambda x: x.mean())\n",
        "\n",
        "# Add a horizontal line for average price\n",
        "plt.axhline(avg_price, color='red', linestyle='dashed', linewidth=2, label=f'Avg Price: {avg_price:.2f}')\n",
        "\n",
        "# Annotate each bar with its mean price\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height():.2f}',\n",
        "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='bottom', fontsize=10, color='black')\n",
        "\n",
        "# Set proper xticks before setting labels\n",
        "ax.set_xticks([0, 1])\n",
        "ax.set_xticklabels([\"Non-Superhost\", \"Superhost\"])\n",
        "\n",
        "# Add title and legend\n",
        "plt.title(\"Average Price by Superhost Status\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wrh3csjkdrpY"
      },
      "outputs": [],
      "source": [
        "# Create bar plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "ax = sns.barplot(x=\"host_identity_verified\", y=\"price\", data=df_cleaned, estimator=lambda x: x.mean())\n",
        "\n",
        "# Add a horizontal line for average price\n",
        "plt.axhline(avg_price, color='red', linestyle='dashed', linewidth=2, label=f'Avg Price: {avg_price:.2f}')\n",
        "\n",
        "# Annotate each bar with its mean price\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height():.2f}',\n",
        "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='bottom', fontsize=10, color='black')\n",
        "\n",
        "# Set proper xticks before setting labels\n",
        "ax.set_xticks([0, 1])\n",
        "ax.set_xticklabels([\"Non-Verified\", \"Verified\"])\n",
        "\n",
        "# Add title and legend\n",
        "plt.title(\"Average Price by Verified Status\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWsV2ITfqhda"
      },
      "source": [
        "## 1.5 Review Attributes\n",
        "\n",
        "Feature processing with features corresponding to reviews given by users for each listing.\n",
        "\n",
        "This conists of :\n",
        "\n",
        "* 'number_of_reviews',\n",
        "* 'review_scores_rating',\n",
        "* 'review_scores_accuracy',\n",
        "* 'review_scores_cleanliness',\n",
        "* 'review_scores_checkin',\n",
        "* 'review_scores_communication',\n",
        "* 'review_scores_location',\n",
        "* 'review_scores_value',\n",
        "* 'reviews_per_month',"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljYnvvBW0Tfg"
      },
      "outputs": [],
      "source": [
        "review_attributes = ['price',\n",
        "'number_of_reviews',\n",
        "'review_scores_rating',\n",
        "'review_scores_accuracy',\n",
        "'review_scores_cleanliness',\n",
        "'review_scores_checkin',\n",
        "'review_scores_communication',\n",
        "'review_scores_location',\n",
        "'review_scores_value',\n",
        "'reviews_per_month']\n",
        "df_cleaned[review_attributes].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJ-rbr56fpi-"
      },
      "outputs": [],
      "source": [
        "# for number of reviews, check the mean and outliers\n",
        "print(df_cleaned.number_of_reviews.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IjZaVighRmi"
      },
      "outputs": [],
      "source": [
        "# normalise the review scores\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Select only the review score columns\n",
        "review_cols = [\n",
        "    \"review_scores_rating\", \"review_scores_accuracy\", \"review_scores_cleanliness\",\n",
        "    \"review_scores_checkin\", \"review_scores_communication\", \"review_scores_location\", \"review_scores_value\"\n",
        "]\n",
        "\n",
        "# Apply MinMaxScaler (0-5 → 0-1)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "df_cleaned[review_cols] = scaler.fit_transform(df_cleaned[review_cols])\n",
        "df_cleaned[review_attributes].head()\n",
        "# df_cleaned_no_outliers[review_cols] = scaler.fit_transform(df_cleaned_no_outliers[review_cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iF01i7whXk_"
      },
      "outputs": [],
      "source": [
        "# check the monthly reviews\n",
        "print(df_cleaned.reviews_per_month.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMBU4BQFh_xd"
      },
      "outputs": [],
      "source": [
        "# heatmap\n",
        "corr_review = df_cleaned[review_attributes].corr()\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.heatmap(corr_review, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ar0YMve0iL9Y"
      },
      "outputs": [],
      "source": [
        "# prices against number of reviews\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.lineplot(data=df_cleaned, x=\"number_of_reviews\", y=\"price\", errorbar=None)\n",
        "plt.xlabel(\"Number of Reviews per Listing\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Price vs. Reviews\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iW3BT1ZmUPL9"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(x=\"review_scores_rating\", y=\"price\", data=df_cleaned, alpha=0.5)\n",
        "plt.title(\"Price vs. Review Score Rating Distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L1K_wLfVAyF"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(x=\"review_scores_cleanliness\", y=\"price\", data=df_cleaned, alpha=0.5)\n",
        "plt.title(\"Price vs. Reviews Scores Cleanliness Distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_caqVg_nVdxS"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(x=\"reviews_per_month\", y=\"price\", data=df_cleaned, alpha=0.5)\n",
        "plt.title(\"Price vs. Reviews Per Month Distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUPwTfxCV5ON"
      },
      "source": [
        "There's not much correlation in this dataset to warrent using the reviews statistics as a main feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDLX9YNCzTrN"
      },
      "source": [
        "## 1.6 AirBnB Booking Attributes\n",
        "\n",
        "This is feature processing related to other factors related to the booking process.\n",
        "\n",
        "This includes:\n",
        "\n",
        "* 'minimum_nights',\n",
        "* 'maximum_nights',\n",
        "* 'minimum_minimum_nights',\n",
        "* 'maximum_minimum_nights',\n",
        "* 'minimum_maximum_nights',\n",
        "* 'maximum_maximum_nights',\n",
        "* 'minimum_nights_avg_ntm',\n",
        "* 'availability_30',\n",
        "* 'availability_60',\n",
        "* 'availability_90',\n",
        "* 'availability_365',\n",
        "* 'instant_bookable',"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e3wLnQH0ezp"
      },
      "outputs": [],
      "source": [
        "booking_attributes = ['minimum_nights',\n",
        "'maximum_nights',\n",
        "'minimum_minimum_nights',\n",
        "'maximum_minimum_nights',\n",
        "'minimum_maximum_nights',\n",
        "'maximum_maximum_nights',\n",
        "'minimum_nights_avg_ntm',\n",
        "'availability_30',\n",
        "'availability_60',\n",
        "'availability_90',\n",
        "'availability_365',\n",
        "'instant_bookable']\n",
        "df_cleaned[booking_attributes].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ey1bAZn1iUsv"
      },
      "outputs": [],
      "source": [
        "# can consider dropping some of the columns here due to redundancy\n",
        "print(df_cleaned[\"minimum_nights\"].unique())\n",
        "print(df_cleaned[\"minimum_nights\"].value_counts())\n",
        "print(df_cleaned[\"maximum_nights\"].unique())\n",
        "print(df_cleaned[\"maximum_nights\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-B58rINPaK63"
      },
      "outputs": [],
      "source": [
        "# check the availabilty columns\n",
        "print(df_cleaned['availability_30'].unique())\n",
        "print(df_cleaned['availability_30'].value_counts())\n",
        "print(df_cleaned['availability_60'].unique())\n",
        "print(df_cleaned['availability_60'].value_counts())\n",
        "print(df_cleaned['availability_90'].unique())\n",
        "print(df_cleaned['availability_90'].value_counts())\n",
        "print(df_cleaned['availability_365'].unique())\n",
        "print(df_cleaned['availability_365'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSXQoz04WmcX"
      },
      "outputs": [],
      "source": [
        "# can drop the min-min nights, max-min nights, min-max nights, max-max nights, kinda redundant\n",
        "# print(len(df_cleaned.columns))\n",
        "# df_cleaned.drop(columns=[\"minimum_minimum_nights\", \"maximum_minimum_nights\", \"minimum_maximum_nights\", \"maximum_maximum_nights\"], inplace=True)\n",
        "# df_cleaned_no_outliers.drop(columns=[\"minimum_minimum_nights\", \"maximum_minimum_nights\", \"minimum_maximum_nights\", \"maximum_maximum_nights\"], inplace=True)\n",
        "# print(len(df_cleaned.columns))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L98UAZO4Xvuz"
      },
      "source": [
        "Leave availabilty as it is. Leave the average nights as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRdi9zffXoKH"
      },
      "outputs": [],
      "source": [
        "# map instant bookable\n",
        "df_cleaned[\"instant_bookable\"] = df_cleaned[\"instant_bookable\"].map({\"t\": True, \"f\": False})\n",
        "# df_cleaned_no_outliers[\"instant_bookable\"] = df_cleaned_no_outliers[\"instant_bookable\"].map({\"t\": True, \"f\": False})\n",
        "df_cleaned[\"instant_bookable\"].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZabrxufXYaCp"
      },
      "outputs": [],
      "source": [
        "# heatmap the shit\n",
        "corr_columns = ['price',\n",
        "'minimum_nights',\n",
        "'maximum_nights',\n",
        "'minimum_nights_avg_ntm',\n",
        "'availability_30',\n",
        "'availability_60',\n",
        "'availability_90',\n",
        "'availability_365',\n",
        "'instant_bookable']\n",
        "\n",
        "corr_host = df_cleaned[corr_columns].corr()\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.heatmap(corr_host, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "\n",
        "# Show plot\n",
        "plt.title(\"Booking Features Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOdd4TTAcsmK"
      },
      "outputs": [],
      "source": [
        "# Check minimum nights graph against price\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.scatterplot(x=df_cleaned[\"minimum_nights\"], y=df_cleaned[\"price\"])\n",
        "plt.title(\"Price vs. Minimum Nights\")\n",
        "plt.xlabel(\"Minimum Nights\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfzn2dPgZW2t"
      },
      "source": [
        "The booking attributes seem to have poor correlated attributes. For now, only the features *avaliabilty_365* and *instant_bookable* seem to be worth looking into."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3i_5_2BbWfF"
      },
      "outputs": [],
      "source": [
        "# analyse availabilty_365\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.scatterplot(x=df_cleaned[\"availability_365\"], y=df_cleaned[\"price\"])\n",
        "plt.title(\"Price vs. Availability (Days per Year)\")\n",
        "plt.xlabel(\"Availability (Days in a Year)\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dnmof7QUcnNx"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "sns.boxplot(x=df_cleaned[\"host_is_superhost\"], y=df_cleaned[\"availability_365\"])\n",
        "plt.title(\"Availability by Superhost Status\")\n",
        "plt.xlabel(\"Superhost\")\n",
        "plt.ylabel(\"Availability (Days per Year)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYWIzhhYcnQr"
      },
      "outputs": [],
      "source": [
        "df_viz = df_cleaned.copy()  # Create a copy to avoid modifying the original data\n",
        "room_col = ['room_type_Entire home/apt','room_type_Hotel room',\t'room_type_Private room',\t'room_type_Shared room']\n",
        "\n",
        "# Convert one-hot encoded columns back to a single categorical column\n",
        "df_viz[\"room_type\"] = df_viz[room_col].idxmax(axis=1).str.replace(\"room_type_\", \"\")\n",
        "\n",
        "# Now plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(x=df_viz[\"room_type\"], y=df_viz[\"availability_365\"])\n",
        "plt.title(\"Availability by Room Type\")\n",
        "plt.xlabel(\"Room Type\")\n",
        "plt.ylabel(\"Availability (Days per Year)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RyQkcute2mm"
      },
      "outputs": [],
      "source": [
        "# See distribution of availabilty\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df_cleaned[\"availability_365\"], bins=30, kde=True)\n",
        "plt.title(\"Distribution of Availability (Days per Year)\")\n",
        "plt.xlabel(\"Availability (Days in a Year)\")\n",
        "plt.ylabel(\"Count of Listings\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DRzkDklchB7"
      },
      "source": [
        "It is hard to see any kind of relationship here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bri6tJd2aD5c"
      },
      "outputs": [],
      "source": [
        "# Create bar plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "ax = sns.barplot(x=\"instant_bookable\", y=\"price\", data=df_cleaned, estimator=lambda x: x.mean())\n",
        "\n",
        "# Add a horizontal line for average price\n",
        "plt.axhline(avg_price, color='red', linestyle='dashed', linewidth=2, label=f'Avg Price: {avg_price:.2f}')\n",
        "\n",
        "# Annotate each bar with its mean price\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height():.2f}',\n",
        "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='bottom', fontsize=10, color='black')\n",
        "\n",
        "# Set proper xticks before setting labels\n",
        "ax.set_xticks([0, 1])\n",
        "ax.set_xticklabels([\"Not Instant Bookable\", \"Is Instant Bookable\"])\n",
        "\n",
        "# Add title and legend\n",
        "plt.title(\"Average Price by Booking Status\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfeEvy1rfzbJ"
      },
      "source": [
        "## 1.7 Final Findings\n",
        "\n",
        "We will be using most of the physical attributes and location attributes. But for host, review and the booking attributes these have little bearing on the price prediction, so it will be removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PL5SzJagfObL"
      },
      "outputs": [],
      "source": [
        "print(df_cleaned.columns)\n",
        "# drop the following\n",
        "drop_list = ['minimum_nights',\n",
        "'maximum_nights',\n",
        "'minimum_minimum_nights',\n",
        "'maximum_minimum_nights',\n",
        "'minimum_maximum_nights',\n",
        "'maximum_maximum_nights',\n",
        "'minimum_nights_avg_ntm',\n",
        "'availability_30',\n",
        "'availability_60',\n",
        "'availability_90',\n",
        "'availability_365',\n",
        "'name',\n",
        "'description',\n",
        "'review_scores_accuracy',\n",
        "'review_scores_cleanliness',\n",
        "'review_scores_checkin',\n",
        "'review_scores_communication',\n",
        "'review_scores_location',\n",
        "'review_scores_value',\n",
        "'host_identity_verified',\n",
        "'amenities',\n",
        "'neighbourhood_cleansed',\n",
        "'longitude',\n",
        "'latitude',\n",
        "'nearest_station',\n",
        "'location_cluster'\n",
        "]\n",
        "\n",
        "df_cleaned = df_cleaned.drop(columns=drop_list)\n",
        "# df_cleaned_no_outliers = df_cleaned_no_outliers.drop(columns=drop_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7N44EisgWaL"
      },
      "outputs": [],
      "source": [
        "print(df_cleaned.columns)\n",
        "print(len(df_cleaned.columns))\n",
        "df_cleaned.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WX7nCHN0GVJq"
      },
      "source": [
        "# 2. Additional Feature Engineering\n",
        "\n",
        "This section will solely for feature engineering more relevant columns to be used and tested. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLhyhEz-E8x8"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Ensure no division by zero (adding +1)\n",
        "df_cleaned[\"room_density\"] = df_cleaned[\"beds\"] / (df_cleaned[\"bedrooms\"] + 1)\n",
        "df_cleaned[\"space_per_bed\"] = df_cleaned[\"accommodates\"] / (df_cleaned[\"beds\"] + 1)\n",
        "\n",
        "# Log Transform (if needed)\n",
        "df_cleaned[\"log_accommodation\"] = np.log1p(df_cleaned[\"accommodates\"])\n",
        "df_cleaned[\"log_bedrooms\"] = np.log1p(df_cleaned[\"bedrooms\"])\n",
        "df_cleaned[\"log_beds\"] = np.log1p(df_cleaned[\"beds\"])\n",
        "\n",
        "# Polynomial Features\n",
        "df_cleaned[\"bedrooms_squared\"] = df_cleaned[\"bedrooms\"] ** 2\n",
        "df_cleaned[\"beds_squared\"] = df_cleaned[\"beds\"] ** 2\n",
        "df_cleaned[\"accommodation_beds_interaction\"] = df_cleaned[\"accommodates\"] * df_cleaned[\"beds\"]\n",
        "\n",
        "# K-Means Clustering (Choose a reasonable number of clusters, e.g., 4)\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "df_cleaned[\"room_cluster\"] = kmeans.fit_predict(df_cleaned[[\"accommodates\", \"bedrooms\", \"beds\"]])\n",
        "\n",
        "engineered_cols = ['price',\n",
        "                   'room_density',\n",
        "                   'space_per_bed',\n",
        "                   'log_accommodation',\n",
        "                   'log_bedrooms',\n",
        "                   'log_beds',\n",
        "                   'bedrooms_squared',\n",
        "                   'beds_squared',\n",
        "                   'accommodation_beds_interaction',\n",
        "                   'room_cluster']\n",
        "\n",
        "# Display the first few rows to check the new features\n",
        "df_cleaned[engineered_cols].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hYYyTWfE9zI"
      },
      "outputs": [],
      "source": [
        "# Compute correlation with target variable (e.g., 'price')\n",
        "corr_matrix = df_cleaned[engineered_cols].corr()\n",
        "\n",
        "# Sort correlations to see the strongest ones\n",
        "corr_with_target = corr_matrix[\"price\"].sort_values(ascending=False)\n",
        "print(corr_with_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfsL4S3nH-KZ"
      },
      "outputs": [],
      "source": [
        "# Drop the least correlated engineered features     \n",
        "df_cleaned.drop(columns=['room_cluster','space_per_bed', 'room_density'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dM2hkNkMQDcs"
      },
      "outputs": [],
      "source": [
        "df_cleaned.columns.to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Outliers**\n",
        "\n",
        "We are going to separate more outliers from the curve and perform analysis to see if the data is actually accurate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check here WIP\n",
        "# Compute KDE (smoothed curve from histogram)\n",
        "kde = sns.kdeplot(df_cleaned[\"price\"], bw_adjust=0.5)\n",
        "\n",
        "# Find the peak (mode) of the distribution\n",
        "peak_price = df_cleaned[\"price\"].mode()[0]\n",
        "\n",
        "# Define threshold dynamically (e.g., 2x peak price)\n",
        "threshold = 2 * peak_price\n",
        "\n",
        "# Split data\n",
        "df_cleaned_no_outliers = df_cleaned[df_cleaned[\"price\"] <= threshold]\n",
        "df_cleaned_outliers = df_cleaned[df_cleaned[\"price\"] > threshold]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# show the changes\n",
        "print(f\"Original cleaned :{df_cleaned.shape[0]}\")\n",
        "print(f\"Outliers removed :{df_cleaned_no_outliers.shape[0]}\")\n",
        "print(f\"Outliers only : {df_cleaned_outliers.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cleaned['price'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glydNrgMGYlO"
      },
      "source": [
        "# 3. Data Fitting and Model Training\n",
        "\n",
        "We will be focused on comparing results from RandomForest and SVM. We have a linear regression model as a baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Linear**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🔹 Split data into training and testing sets (80-20 split)\n",
        "linear_X = df_cleaned.drop(columns='price')  # Features\n",
        "linear_y = df_cleaned['price']  # Target Variable\n",
        "\n",
        "linear_X_train, linear_X_test, linear_y_train, linear_y_test = train_test_split(\n",
        "    linear_X, linear_y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 🔹 Initialize and train Linear Regression model\n",
        "lr_model = LinearRegression()\n",
        "# Start timer\n",
        "start_time = time.time()\n",
        "# train\n",
        "lr_model.fit(linear_X_train, linear_y_train)\n",
        "# End timer\n",
        "train_time_lr = time.time() - start_time\n",
        "\n",
        "# 🔹 Make predictions\n",
        "start_time = time.time()\n",
        "linear_y_pred_lr = lr_model.predict(linear_X_test)\n",
        "# End timer\n",
        "predict_time_lr= time.time() - start_time\n",
        "\n",
        "# 🔹 Evaluate the model\n",
        "mae_lr = mean_absolute_error(linear_y_test, linear_y_pred_lr)\n",
        "mse_lr = mean_squared_error(linear_y_test, linear_y_pred_lr)\n",
        "rmse_lr = np.sqrt(mse_lr)\n",
        "r2_lr = r2_score(linear_y_test, linear_y_pred_lr)\n",
        "\n",
        "# 🔹 Print Evaluation Metrics\n",
        "print(f\"Linear Regression Results:\")\n",
        "print(f\"MAE: {mae_lr:.2f}\")\n",
        "print(f\"MSE: {mse_lr:.2f}\")\n",
        "print(f\"RMSE: {rmse_lr:.2f}\")\n",
        "print(f\"R² Score: {r2_lr:.4f}\")\n",
        "print(f\"Training Time: {train_time_lr:.4f} seconds\")\n",
        "print(f\"Prediction Time: {predict_time_lr:.6f} seconds\")\n",
        "\n",
        "# 🔹 Plot Actual vs Predicted Values\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=linear_y_test, y=linear_y_pred_lr, alpha=0.5)\n",
        "plt.plot([linear_y_test.min(), linear_y_test.max()],\n",
        "         [linear_y_test.min(), linear_y_test.max()],\n",
        "         color='red', linestyle='dashed')  # Perfect predictions line\n",
        "plt.xlabel(\"Actual Prices\")\n",
        "plt.ylabel(\"Predicted Prices\")\n",
        "plt.title(\"Linear Regression: Actual vs Predicted Prices\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIkHPbsI2nYN"
      },
      "source": [
        "## **SVM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fitting\n",
        "# X = df_cleaned.drop(columns=['price'])\n",
        "# y = df_cleaned['price']\n",
        "X = df_cleaned_no_outliers.drop(columns=['price'])\n",
        "y = df_cleaned_no_outliers['price']\n",
        "\n",
        "print(\"Features shape:\", X.shape, \"Target shape:\", y.shape)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Test set shape:\", X_test.shape)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)    \n",
        "print(\"Training set shape (scaled):\", X_train_scaled.shape)\n",
        "print(\"Test set shape (scaled):\", X_test_scaled.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SVM Kernel Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    \"\"\"Compute and print evaluation metrics for a model.\"\"\"\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    \n",
        "    print(f\"\\n📊 {model_name} Performance Metrics:\")\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "    \n",
        "    return r2, mae, rmse\n",
        "\n",
        "############################################################\n",
        "# 1. Kernel Comparison Setup\n",
        "############################################################\n",
        "\n",
        "# List of kernels to evaluate\n",
        "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "\n",
        "# Store results for comparison\n",
        "comparison_results = []\n",
        "\n",
        "############################################################\n",
        "# 2. Compare SVM Kernels\n",
        "############################################################\n",
        "for k in kernels:\n",
        "    print(f\"\\n--- Evaluating SVM with '{k}' kernel ---\")\n",
        "    \n",
        "    # Create the SVR model\n",
        "    # - For the polynomial kernel, we explicitly set degree=3.\n",
        "    # - For others, we keep default parameters except C=1.0 for consistency.\n",
        "    if k == 'poly':\n",
        "        svm_model = SVR(kernel=k, C=1.0, degree=3)\n",
        "    else:\n",
        "        svm_model = SVR(kernel=k, C=1.0)\n",
        "    \n",
        "    start_time = time.time()\n",
        "    # Train the model\n",
        "    svm_model.fit(X_train_scaled, y_train)\n",
        "    train_time = start_time - time.time()\n",
        "\n",
        "\n",
        "    # Predict on test data\n",
        "    start_time =  time.time()\n",
        "    y_pred = svm_model.predict(X_test_scaled)\n",
        "    predict_time = start_time - time.time()\n",
        "\n",
        "    # Evaluate the model using your custom function\n",
        "    r2, mae, rmse = evaluate_model(y_test, y_pred, f\"SVM ({k} kernel)\")\n",
        "    \n",
        "    # Append results in a list for each kernel\n",
        "    comparison_results.append([k, mae, rmse, r2, train_time, predict_time])\n",
        "\n",
        "############################################################\n",
        "# 3. Create and Print Comparison Table (Without MSE)\n",
        "############################################################\n",
        "df_kernel_comparison = pd.DataFrame(\n",
        "    comparison_results,\n",
        "    columns=[\"Kernel\", \"MAE\", \"RMSE\", \"R² Score\", \"Training Time\", \"Prediction Time\"]\n",
        ")\n",
        "\n",
        "print(\"\\nSVM Kernel Comparison Results:\")\n",
        "print(df_kernel_comparison)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Baseline SVM (linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVR\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from skopt import BayesSearchCV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    \"\"\"Compute and print evaluation metrics for a model.\"\"\"\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    print(f\"\\n📊 {model_name} Performance Metrics:\")\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "    return r2, mae, rmse\n",
        "\n",
        "\n",
        "print(\"\\n--- 5.1 Baseline SVM (Linear) ---\")\n",
        "# Train SVR with linear kernel on scaled data\n",
        "svm_linear = SVR(kernel='linear', C=1.0)\n",
        "start_time = time.time()\n",
        "svm_linear.fit(X_train_scaled, y_train)\n",
        "train_time_svm_liner = time.time() - start_time \n",
        "\n",
        "start_time = time.time()\n",
        "y_pred_linear = svm_linear.predict(X_test_scaled)\n",
        "predict_time_svm_linear = time.time() - start_time\n",
        "\n",
        "r2_base, mae_base, rmse_base = evaluate_model(y_test, y_pred_linear, \"Baseline SVM (Linear)\")\n",
        "print(f\"Training Time: {train_time_svm_liner:.4f} seconds\")\n",
        "print(f\"Prediction Time: {predict_time_svm_linear:.6f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bayesian Tuned SVM (Linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- 5.2 Bayesian Tuned SVM (Linear) ---\")\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "# Define hyperparameter search space\n",
        "param_space = {\n",
        "    'C': (0.1, 100.0, 'log-uniform'),\n",
        "    'epsilon': (0.01, 10.0, 'log-uniform')\n",
        "}\n",
        "# Bayesian optimization for linear SVM with reduced iterations and CV\n",
        "svm_linear_bayes = BayesSearchCV(\n",
        "    estimator=SVR(kernel='linear'),\n",
        "    search_spaces=param_space,\n",
        "    n_iter=5,\n",
        "    cv=2,\n",
        "    scoring='neg_mean_absolute_error',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "start_time = time.time()\n",
        "svm_linear_bayes.fit(X_train_scaled, y_train)\n",
        "train_time_baye_svm_lr = time.time() - start_time\n",
        "print(\"Best parameters for linear SVM:\", svm_linear_bayes.best_params_)\n",
        "\n",
        "start_time = time.time()\n",
        "y_pred_linear_bayes = svm_linear_bayes.predict(X_test_scaled)\n",
        "predict_time_baye_svm_linear = time.time() - start_time\n",
        "\n",
        "r2_bayes, mae_bayes, rmse_bayes = evaluate_model(y_test, y_pred_linear_bayes, \"Bayesian Tuned SVM (Linear)\")\n",
        "print(f\"Training Time: {train_time_baye_svm_lr:.4f} seconds\")\n",
        "print(f\"Prediction Time: {predict_time_baye_svm_linear:.6f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Blended Model (SVM + XGBoost + LightGBM) with Scaled Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- 5.3 Blended Model (SVM+XGB+LGB) ---\")\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "# # Train a baseline SVM (linear kernel) on scaled data\n",
        "# svm_model = SVR(kernel='linear', C=1.0)\n",
        "# svm_model.fit(X_train_scaled, y_train)\n",
        "# y_pred_svm = svm_model.predict(X_test_scaled)\n",
        "\n",
        "# # Train XGBoost\n",
        "# xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "# xgb_model.fit(X_train_scaled, y_train)\n",
        "# y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
        "\n",
        "# # Train LightGBM\n",
        "# lgb_model = LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "# lgb_model.fit(X_train_scaled, y_train)\n",
        "# y_pred_lgb = lgb_model.predict(X_test_scaled)\n",
        "\n",
        "# # Blend predictions (simple average)\n",
        "# y_pred_blend = (y_pred_svm + y_pred_xgb + y_pred_lgb) / 3\n",
        "# r2_blend, mae_blend, rmse_blend = evaluate_model(y_test, y_pred_blend, \"Blended Model (SVM+XGB+LGB)\")\n",
        "# Track total training time\n",
        "start_time = time.time()\n",
        "\n",
        "# Train SVM\n",
        "svm_start = time.time()\n",
        "svm_model = SVR(kernel='linear', C=1.0)\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "svm_train_time = time.time() - svm_start\n",
        "\n",
        "# Train XGBoost\n",
        "xgb_start = time.time()\n",
        "xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "xgb_train_time = time.time() - xgb_start\n",
        "\n",
        "# Train LightGBM\n",
        "lgb_start = time.time()\n",
        "lgb_model = LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "lgb_model.fit(X_train_scaled, y_train)\n",
        "lgb_train_time = time.time() - lgb_start\n",
        "\n",
        "total_train_time = time.time() - start_time\n",
        "\n",
        "# Track total prediction time\n",
        "pred_start = time.time()\n",
        "\n",
        "# Predict SVM\n",
        "svm_pred_start = time.time()\n",
        "y_pred_svm = svm_model.predict(X_test_scaled)\n",
        "svm_pred_time = time.time() - svm_pred_start\n",
        "\n",
        "# Predict XGBoost\n",
        "xgb_pred_start = time.time()\n",
        "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
        "xgb_pred_time = time.time() - xgb_pred_start\n",
        "\n",
        "# Predict LightGBM\n",
        "lgb_pred_start = time.time()\n",
        "y_pred_lgb = lgb_model.predict(X_test_scaled)\n",
        "lgb_pred_time = time.time() - lgb_pred_start\n",
        "\n",
        "# Blended Prediction\n",
        "blend_pred_start = time.time()\n",
        "y_pred_blend = (y_pred_svm + y_pred_xgb + y_pred_lgb) / 3\n",
        "blend_pred_time = time.time() - blend_pred_start\n",
        "\n",
        "total_pred_time = time.time() - pred_start\n",
        "\n",
        "# Evaluate blended model\n",
        "r2_blend, mae_blend, rmse_blend = evaluate_model(y_test, y_pred_blend, \"Blended Model (SVM+XGB+LGB)\")\n",
        "\n",
        "# Print training and prediction times\n",
        "print(\"\\n--- Training Times ---\")\n",
        "print(f\"SVM Training Time: {svm_train_time:.4f} sec\")\n",
        "print(f\"XGBoost Training Time: {xgb_train_time:.4f} sec\")\n",
        "print(f\"LightGBM Training Time: {lgb_train_time:.4f} sec\")\n",
        "print(f\"Total Training Time: {total_train_time:.4f} sec\")\n",
        "\n",
        "print(\"\\n--- Prediction Times ---\")\n",
        "print(f\"SVM Prediction Time: {svm_pred_time:.4f} sec\")\n",
        "print(f\"XGBoost Prediction Time: {xgb_pred_time:.4f} sec\")\n",
        "print(f\"LightGBM Prediction Time: {lgb_pred_time:.4f} sec\")\n",
        "print(f\"Blended Prediction Time: {blend_pred_time:.4f} sec\")\n",
        "print(f\"Total Prediction Time: {total_pred_time:.4f} sec\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Advanced Stacking (SVM + XGBoost + LightGBM + Ridge) with Scaled Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- 5.4 Advanced Stacking (SVM+XGB+LGB+Ridge) ---\")\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Define base estimators (without RF)\n",
        "estimators = [\n",
        "    ('svm', SVR(kernel='linear', C=1.0)),\n",
        "    ('xgb', XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)),\n",
        "    ('lgb', LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42))\n",
        "]\n",
        "# Stacking regressor with Ridge as meta-learner\n",
        "stacking_reg = StackingRegressor(\n",
        "    estimators=estimators,\n",
        "    final_estimator=Ridge(),\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "start_time = time.time()\n",
        "stacking_reg.fit(X_train_scaled, y_train)\n",
        "train_time_stack_svm =  time.time() - start_time\n",
        "start_time = time.time()\n",
        "y_pred_stack = stacking_reg.predict(X_test_scaled)\n",
        "predict_time_stack_svm = time.time() - start_time\n",
        "r2_stack, mae_stack, rmse_stack = evaluate_model(y_test, y_pred_stack, \"Advanced Stacking (SVM+XGB+LGB+Ridge)\")\n",
        "print(f\"Training Time: {train_time_stack_svm:.4f} seconds\")\n",
        "print(f\"Prediction Time: {predict_time_stack_svm:.6f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # ✅ Feature Importance (Optional)\n",
        "# plt.figure(figsize=(15,9))\n",
        "# sns.barplot(x=xgboost_model.feature_importances_, y=treeXG_X.columns)\n",
        "# plt.xlabel(\"Feature Importance\")\n",
        "# plt.ylabel(\"Feature Name\")\n",
        "# plt.title(\"XGBoost Feature Importance\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results For SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# 6. Summary of Results\n",
        "# -------------------------------\n",
        "print(\"\\n=== Summary of Model Performance ===\")\n",
        "results_summary = pd.DataFrame({\n",
        "    'Model': [\n",
        "        'Baseline SVM (Linear)',\n",
        "        'Bayesian Tuned SVM (Linear)',\n",
        "        'Blended Model (SVM+XGB+LGB)',\n",
        "        'Advanced Stacking (SVM+XGB+LGB+Ridge)'\n",
        "    ],\n",
        "    'R² Score': [r2_base, r2_bayes, r2_blend, r2_stack],\n",
        "    'MAE': [mae_base, mae_bayes, mae_blend, mae_stack],\n",
        "    'RMSE': [rmse_base, rmse_bayes, rmse_blend, rmse_stack],\n",
        "    'Training Time (Seconds)': [train_time_svm_liner, train_time_baye_svm_lr, total_train_time, train_time_stack_svm],\n",
        "    'Prediction Time (Seconds)': [predict_time_svm_linear, predict_time_baye_svm_linear, total_pred_time, predict_time_stack_svm]\n",
        "})\n",
        "print(results_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chris basement WIP\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "#Basic splitting\n",
        "tree_X = df_cleaned.drop(columns='price')  # Features\n",
        "tree_y = df_cleaned['price']  # Target Variable\n",
        "\n",
        "# Split into 80% training and 20% testing data\n",
        "tree_X_train, tree_X_test, tree_y_train, tree_y_test = train_test_split(tree_X, tree_y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "#training of the model\n",
        "# Initialize Decision Tree Regressor\n",
        "tree_model = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "start_time = time.time()\n",
        "tree_model.fit(tree_X_train, tree_y_train)\n",
        "train_time_base_tree = time.time() - start_time\n",
        "\n",
        "\n",
        "#making prediction\n",
        "#chris_y_pred = model.predict(X_test)\n",
        "start_time = time.time()\n",
        "tree_y_pred = tree_model.predict(tree_X_test)\n",
        "predict_time_base_tree = time.time() - start_time\n",
        "print(\"Predicted Prices:\", tree_y_pred)\n",
        "\n",
        "\n",
        "#evaluation of the modeL?\n",
        "tree_mae = mean_absolute_error(tree_y_test, tree_y_pred)\n",
        "tree_mse = mean_squared_error(tree_y_test, tree_y_pred)\n",
        "tree_rmse = np.sqrt(tree_mse)\n",
        "tree_r2 = r2_score(tree_y_test, tree_y_pred)\n",
        "\n",
        "print(f\"Mean Absolute Error (MAE): {tree_mae}\")\n",
        "print(f\"Mean Squared Error (MSE): {tree_mse}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {tree_rmse}\")\n",
        "print(f\"R² Score: {tree_r2}\")\n",
        "print(f\"Training Time: {train_time_base_tree:.4f} seconds\")\n",
        "print(f\"Prediction Time: {predict_time_base_tree:.6f} seconds\")\n",
        "\n",
        "\n",
        "#tree plotting?\n",
        "\n",
        "from sklearn.tree import plot_tree\n",
        "\n",
        "plt.figure(figsize=(30,30))\n",
        "plot_tree(tree_model, feature_names=tree_X.columns, filled=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Forest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# ✅ Load & Prepare Data (Assuming df_cleaned_no_outliers is your cleaned dataset)\n",
        "tree_X = df_cleaned.drop(columns='price')  # Features\n",
        "tree_y = df_cleaned['price']  # Target Variable\n",
        "\n",
        "# Split into training and testing sets\n",
        "tree_X_train, tree_X_test, tree_y_train, tree_y_test = train_test_split(\n",
        "    tree_X, tree_y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ✅ Initialize & Train Random Forest Model\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=100,  # Number of trees in the forest\n",
        "    max_depth=10,      # Limits tree depth to prevent overfitting\n",
        "    random_state=42,\n",
        "    n_jobs=-1          # Uses all CPU cores for faster training\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "rf_model.fit(tree_X_train, tree_y_train)\n",
        "train_time_rf = time.time() - start_time\n",
        "\n",
        "# ✅ Make Predictions\n",
        "start_time = time.time()\n",
        "y_pred_rf = rf_model.predict(tree_X_test)\n",
        "predict_time_rf = time.time() - start_time\n",
        "\n",
        "# ✅ Evaluate Performance\n",
        "mae_rf = mean_absolute_error(tree_y_test, y_pred_rf)\n",
        "mse_rf = mean_squared_error(tree_y_test, y_pred_rf)\n",
        "rmse_rf = np.sqrt(mse_rf)  # RMSE is the square root of MSE\n",
        "r2_rf = r2_score(tree_y_test, y_pred_rf)\n",
        "\n",
        "# ✅ Print Results\n",
        "print(f\"Random Forest - Mean Absolute Error (MAE): {mae_rf}\")\n",
        "print(f\"Random Forest - Mean Squared Error (MSE): {mse_rf}\")\n",
        "print(f\"Random Forest - Root Mean Squared Error (RMSE): {rmse_rf}\")\n",
        "print(f\"Random Forest - R² Score: {r2_rf:.4f}\")\n",
        "print(f\"Training Time: {train_time_rf:.4f} seconds\")\n",
        "print(f\"Prediction Time: {predict_time_rf:.6f} seconds\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### With XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from xgboost import XGBRegressor  # Import XGBoost\n",
        "\n",
        "# Basic data splitting\n",
        "treeXG_X = df_cleaned.drop(columns='price')  # Features\n",
        "treeXG_y = df_cleaned['price']  # Target Variable\n",
        "\n",
        "low_importance_features = ['has_baby_bath', 'has_bbq_grill', 'has_game_console', 'has_exercise_equipment']\n",
        "treeXG_X = treeXG_X.drop(columns=low_importance_features)\n",
        "\n",
        "\n",
        "# Split into 80% training and 20% testing data\n",
        "treeXG_X_train, treeXG_X_test, treeXG_y_train, treeXG_y_test = train_test_split(\n",
        "    treeXG_X, treeXG_y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ✅ Initialize XGBoost Regressor\n",
        "\n",
        "\n",
        "xgboost_model = XGBRegressor(\n",
        "    n_estimators=3000,  # 🔼 Increase estimators for better learning\n",
        "    learning_rate=0.03,  # 🔽 Lower learning rate for better convergence\n",
        "    max_depth=10,  # 🔽 Reduce depth to prevent overfitting\n",
        "    subsample=0.8,  # ✅ Helps prevent overfitting\n",
        "    colsample_bytree=0.8,  # ✅ Use only 80% of features per tree\n",
        "    gamma=0.1,  # ✅ Adds regularization\n",
        "    reg_lambda=2,  # ✅ L2 Regularization (helps generalization)\n",
        "    reg_alpha=1,  # ✅ L1 Regularization (helps with feature selection)\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ✅ Train the model\n",
        "start_time = time.time()\n",
        "xgboost_model.fit(treeXG_X_train, treeXG_y_train)\n",
        "train_time_xgb = time.time() - start_time\n",
        "\n",
        "\n",
        "# ✅ Make predictions\n",
        "start_time = time.time()\n",
        "treeXG_y_pred_xgb = xgboost_model.predict(treeXG_X_test)\n",
        "predict_time_xgb = time.time() - start_time\n",
        "print(\"Predicted Prices (XGBoost):\", treeXG_y_pred_xgb)\n",
        "\n",
        "# ✅ Evaluate the model\n",
        "mae_xgb = mean_absolute_error(treeXG_y_test, treeXG_y_pred_xgb)\n",
        "mse_xgb = mean_squared_error(treeXG_y_test, treeXG_y_pred_xgb)\n",
        "rmse_xgb = np.sqrt(mse_xgb)\n",
        "r2_xgb = r2_score(treeXG_y_test, treeXG_y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost - Mean Absolute Error (MAE): {mae_xgb}\")\n",
        "print(f\"XGBoost - Mean Squared Error (MSE): {mse_xgb}\")\n",
        "print(f\"XGBoost - Root Mean Squared Error (RMSE): {rmse_xgb}\")\n",
        "print(f\"XGBoost - R² Score: {r2_xgb}\")\n",
        "print(f\"Training Time: {train_time_xgb:.4f} seconds\")\n",
        "print(f\"Prediction Time: {predict_time_xgb:.6f} seconds\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "\n",
        "treeLGBM_X = df_cleaned.drop(columns='price')  # Features\n",
        "treeLGBM_y = df_cleaned['price']  # Target Variable\n",
        "\n",
        "\n",
        "treeXG_X['price_per_accommodate'] = treeXG_y / treeXG_X['accommodates']\n",
        "\n",
        "\n",
        "\n",
        "treeLGBM_X_train, treeLGBM_X_test, treeLGBM_y_train, treeLGBM_y_test = train_test_split(\n",
        "    treeLGBM_X, treeLGBM_y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "lgbm_model = LGBMRegressor(\n",
        "    n_estimators=700,  # Increase trees\n",
        "    learning_rate=0.02,  # Lower learning rate\n",
        "    max_depth=9,  # Allow deeper splits\n",
        "    num_leaves=40,  # Increase complexity slightly\n",
        "    feature_fraction=0.8,  # Use 80% of features per iteration\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "lgbm_model.fit(treeLGBM_X_train, treeLGBM_y_train)\n",
        "train_time_lgb = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "treeLGBM_y_pred = lgbm_model.predict(treeLGBM_X_test)\n",
        "predict_time_lgb = time.time() - start_time\n",
        "print(\"Predicted Prices (LightGBM):\", treeLGBM_y_pred)\n",
        "\n",
        "\n",
        "mae_lgbm = mean_absolute_error(treeLGBM_y_test, treeLGBM_y_pred)\n",
        "mse_lgbm = mean_squared_error(treeLGBM_y_test, treeLGBM_y_pred)\n",
        "rmse_lgbm = np.sqrt(mse_lgbm)\n",
        "r2_lgbm = r2_score(treeLGBM_y_test, treeLGBM_y_pred)\n",
        "\n",
        "print(f\"\\n🔍 Model Performance (LightGBM):\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_lgbm:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_lgbm:.2f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse_lgbm:.2f}\")\n",
        "print(f\"R² Score: {r2_lgbm:.4f}\")\n",
        "print(f\"Training Time: {train_time_lgb:.4f} seconds\")\n",
        "print(f\"Prediction Time: {predict_time_lgb:.6f} seconds\")\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "importance = pd.Series(lgbm_model.feature_importances_, index=treeLGBM_X.columns)\n",
        "importance.nlargest(20).plot(kind='barh')\n",
        "plt.title(\"LightGBM Feature Importance\")\n",
        "plt.xlabel(\"Feature Importance Score\")\n",
        "plt.ylabel(\"Feature Name\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LightGBM with XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.linear_model import Ridge  # Meta-model\n",
        "\n",
        "# ✅ Prepare Data\n",
        "T = df_cleaned.drop(columns='price')\n",
        "U = df_cleaned['price']\n",
        "\n",
        "# Remove low-importance features\n",
        "low_importance_features = ['has_stove', 'has_heating', 'has_game_console', 'has_bbq_grill']\n",
        "T = T.drop(columns=low_importance_features)\n",
        "\n",
        "# Split into training and testing sets\n",
        "t_train, t_test, u_train, u_test = train_test_split(T, U, test_size=0.2, random_state=42)\n",
        "\n",
        "# ✅ Define Base Models\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators=3000, learning_rate=0.02, max_depth=10, subsample=0.9,\n",
        "    colsample_bytree=0.9, gamma=0.2, reg_lambda=3, reg_alpha=2, min_child_weight=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "lgbm_model = LGBMRegressor(\n",
        "    n_estimators=3000, learning_rate=0.02, max_depth=10, subsample=0.9,\n",
        "    colsample_bytree=0.9, reg_lambda=3, reg_alpha=2, random_state=42\n",
        ")\n",
        "\n",
        "# ✅ Define Meta-Model (Final Layer)\n",
        "meta_model = Ridge(alpha=1.0)  # You can try other models like RandomForestRegressor\n",
        "\n",
        "# ✅ Create Stacking Regressor\n",
        "stacking_model = StackingRegressor(\n",
        "    estimators=[('xgb', xgb_model), ('lgbm', lgbm_model)],\n",
        "    final_estimator=meta_model\n",
        ")\n",
        "\n",
        "# Train the Stacking Model (XGBoost + LightGBM)\n",
        "start_time = time.time()\n",
        "stacking_model.fit(t_train, u_train)\n",
        "train_time_xgb_lgb = time.time() - start_time\n",
        "\n",
        "# Make Predictions\n",
        "start_time = time.time()\n",
        "u_pred = stacking_model.predict(t_test)\n",
        "predict_time_xgb_lgb = time.time() - start_time\n",
        "\n",
        "# Evaluate Performance\n",
        "xgb_lgb_mae = mean_absolute_error(u_test, u_pred)\n",
        "xgb_lgb_mse = mean_squared_error(u_test, u_pred)\n",
        "xgb_lgb_rmse = np.sqrt(xgb_lgb_mse)\n",
        "xgb_lgb_r2 = r2_score(u_test, u_pred)\n",
        "\n",
        "print(f\"Stacking - Mean Absolute Error (MAE): {xgb_lgb_mae}\")\n",
        "print(f\"Stacking - Mean Squared Error (MSE): {xgb_lgb_mse}\")\n",
        "print(f\"Stacking - Root Mean Squared Error (RMSE): {xgb_lgb_rmse}\")\n",
        "print(f\"Stacking - R² Score: {r2:.4f}\")\n",
        "print(f\"Training Time: {train_time_xgb_lgb:.4f} seconds\")\n",
        "print(f\"Prediction Time: {predict_time_xgb_lgb:.6f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results for Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n=== Summary of Model Performance ===\")\n",
        "results_summary_tree = pd.DataFrame({\n",
        "    'Model': [\n",
        "        'Tree',\n",
        "        'Random Forest',\n",
        "        'RF + XGBoost',\n",
        "        'RF + LGB',\n",
        "        'RF + XGBoost + LGB'\n",
        "    ],\n",
        "    'R² Score': [tree_r2, r2_rf, r2_xgb, r2_lgbm, xgb_lgb_r2],\n",
        "    'MAE': [tree_mae, mae_rf, mae_xgb, mae_lgbm, xgb_lgb_mae],\n",
        "    'RMSE': [tree_rmse, rmse_rf, rmse_xgb, rmse_lgbm, xgb_lgb_rmse],\n",
        "    'Training Time (Seconds)': [train_time_base_tree, train_time_rf, train_time_xgb, train_time_lgb, train_time_xgb_lgb],\n",
        "    'Prediction Time (Seconds)': [predict_time_base_tree, predict_time_rf, predict_time_xgb, predict_time_lgb, predict_time_xgb_lgb]\n",
        "})\n",
        "results_summary_tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMdvvfvvGfjE"
      },
      "source": [
        "# 4. Analysis and Findings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'results_summary' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mresults_summary\u001b[49m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'results_summary' is not defined"
          ]
        }
      ],
      "source": [
        "results_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_summary_tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing on Stacked SVM\n",
        "random_idx = np.random.randint(0, len(X_test_scaled))\n",
        "sample_X = X_test_scaled[random_idx].reshape(1, -1)\n",
        "\n",
        "# Predict\n",
        "predicted_y = stacking_reg.predict(sample_X)\n",
        "print(f\"Predicted value: {predicted_y[0]}\")\n",
        "print(f\"Actual value: {y_test.iloc[random_idx]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing Stacked Random Forest Tree\n",
        "random_idx = np.random.randint(0, len(u_test))\n",
        "\n",
        "sample_X = t_test.iloc[random_idx].values.reshape(1, -1)\n",
        "predicted_y = stacking_model.predict(sample_X)\n",
        "\n",
        "# Get the actual value\n",
        "actual_y = u_test.iloc[random_idx]\n",
        "\n",
        "print(f\"Predicted value: {predicted_y[0]}\")\n",
        "print(f\"Actual value: {actual_y}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "According to our training, it seems like the best model is the Random Forest stacked with XGBoost and LGB."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## SVM-Based Models\n",
        "\n",
        "| **Model**                                  | **R²**   | **MAE**   | **RMSE**   | **Train (s)** | **Predict (s)** |\n",
        "|-------------------------------------------:|:--------:|:--------:|:---------:|:-------------:|:---------------:|\n",
        "| Baseline SVM (Linear)                      | 0.62     | 37.69     | 54.46     | 69.38         | 10.23           |\n",
        "| Bayesian Tuned SVM (Linear)                | 0.62     | 37.71     | 54.39     | 1067.24       | 13.33           |\n",
        "| Blended Model (SVM + XGB + LGB)            | **0.71** | 32.69     | 47.21     | 73.33         | 14.56           |\n",
        "| Advanced Stacking (SVM + XGB + LGB + Ridge)| **0.73** | **31.88** | **45.93** | 158.31        | 14.47           |\n",
        "\n",
        "<sub>**Bold** indicates the best metric among SVM-based models.</sub>\n",
        "\n",
        "---\n",
        "\n",
        "## Tree-Based Models\n",
        "\n",
        "| **Model**         | **R²**   | **MAE**   | **RMSE**   | **Train (s)** | **Predict (s)** |\n",
        "|:------------------|:--------:|:--------:|:---------:|:-------------:|:---------------:|\n",
        "| Tree              | 0.63     | 36.61     | 53.17     | **0.78**      | **0.01**        |\n",
        "| Random Forest     | 0.70     | 33.84     | 48.30     | 4.54          | 0.04            |\n",
        "| RF + XGBoost      | **0.75** | **29.89** | 43.95     | 24.96         | 0.30            |\n",
        "| RF + LGB          | 0.73     | 31.59     | 45.57     | 1.21          | 0.07            |\n",
        "| RF + XGBoost + LGB| **0.75** | 29.94     | **43.93** | 159.84        | 0.43            |\n",
        "\n",
        "<sub>**Bold** indicates the best metric among tree-based models.</sub>\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
