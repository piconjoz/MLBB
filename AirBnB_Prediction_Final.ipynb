{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou58DdcYK1Ot"
      },
      "source": [
        "# AirBnB Dataset Prediction\n",
        "\n",
        "Testing linear regression (and 1 other algorithm) for predicting price of an AirBnB listing based on various features such as amenities, location and other factors.\n",
        "\n",
        "The dataset used is from https://insideairbnb.com/get-the-data/. Credits to author.\n",
        "\n",
        "3 main cities will be chosen for comparison.\n",
        "\n",
        "They are Singapore, Tokyo Japan, and London England.\n",
        "\n",
        "**Stick to Japan Dataset as of now**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si8BWmhAgeOT"
      },
      "source": [
        "**What the Dataset Columns Entail**\n",
        "\n",
        "https://docs.google.com/spreadsheets/d/1iWCNJcSutYqpULSQHlNyGInUvHg2BoUGoNRIGa6Szc4/edit?gid=1322284596#gid=1322284596"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_VRIwgTLyNm"
      },
      "outputs": [],
      "source": [
        "# AIRBNB PRICE PREDICTION ANALYSIS\n",
        "import os\n",
        "\n",
        "cwd = os.getcwd()\n",
        "print(cwd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFAtGbLzNApX"
      },
      "outputs": [],
      "source": [
        "# move the Airbnb Dataset .gzip file to /content after downloading it at the website\n",
        "import gzip\n",
        "import shutil\n",
        "\n",
        "# download the gzip file from the link above\n",
        "\n",
        "compressed_name = 'listings.csv.gz' # what ever name you choose for the gzip file\n",
        "file_name = 'listings.csv'\n",
        "\n",
        "with gzip.open(compressed_name, 'rb') as f_in:\n",
        "    with open(file_name, 'wb') as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji7mCdT-MA0T"
      },
      "source": [
        "The name of the file will be *listings.csv*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xIdCroVNljT"
      },
      "source": [
        "# 1. Data Cleaning and Preprocessing\n",
        "Clean the data and analyse the features for the most relevant data to be fed to the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_sJJhtaMIdU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# show the three rows\n",
        "# df = pd.read_csv('/content/listings.csv')\n",
        "df = pd.read_csv('listings.csv')\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2t9mlR1MPWPo"
      },
      "outputs": [],
      "source": [
        "# View the columns to see what's available in the dataset\n",
        "print(df.columns)\n",
        "print(len(df.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIPR_BWFPgIK"
      },
      "outputs": [],
      "source": [
        "# view what data types exists in the dataset\n",
        "# brief overview (uncomment to see)\n",
        "# df.dtypes\n",
        "\n",
        "# detailed overview\n",
        "df.info(verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCdd3xqVPp7c"
      },
      "outputs": [],
      "source": [
        "# How many rows are there before cleaning\n",
        "df.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcpjdIAqQG2N"
      },
      "outputs": [],
      "source": [
        "# Determine if there are null values within the dataset\n",
        "# isnull.().sum(axis=0) does not show everything\n",
        "# df.isnull().sum(axis=0)\n",
        "\n",
        "# rather than check everything, just view what columns have null values\n",
        "null_counts = df.isnull().sum()\n",
        "null_counts = null_counts[null_counts > 0]  # Filter only columns with nulls\n",
        "print(null_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRxFFuVWRcFd"
      },
      "source": [
        "We need to drop any unnecessary information that is not relevant to price calculation.\n",
        "\n",
        "We can start first with any irrelevant data that has no impact on price calculation whatsoever."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxz9IFpTS3SC"
      },
      "source": [
        "We want to ensure these columns remain :\n",
        "\n",
        "**Property & Listing Attributes**\n",
        "\n",
        "* property_type (Different types have different pricing, e.g., \"Apartment\" vs. \"Villa\")\n",
        "* room_type (Entire home/apt vs. private room affects price)\n",
        "* accommodates (More guests usually means higher price)\n",
        "* bathrooms_text (Number of bathrooms affects price)\n",
        "* bedrooms (More bedrooms typically increase price)\n",
        "* beds (More beds can increase listing value)\n",
        "* amenities (Luxury amenities like pools, WiFi, and AC may increase price)\n",
        "\n",
        "\n",
        "**Location**\n",
        "* latitude / longitude (Can be used to extract geo-based price trends)\n",
        "\n",
        "**Host Information** (If Significant)\n",
        "* host_is_superhost (Superhosts may charge higher prices)\n",
        "* host_listings_count / host_total_listings_count (Professional hosts vs. casual hosts may price differently)\n",
        "* host_identity_verified (Could be a trust factor affecting pricing)\n",
        "Availability & Minimum Nights\n",
        "* minimum_nights (Longer stays could impact price)\n",
        "* availability_30, availability_60, availability_90, availability_365 (More * * availability may suggest demand or pricing trends)\n",
        "\n",
        "**Reviews & Ratings**\n",
        "* number_of_reviews (More reviews may indicate demand)\n",
        "* review_scores_rating (Higher ratings may allow higher prices)\n",
        "* reviews_per_month (Shows frequency of bookings, indicating demand)\n",
        "\n",
        "Edit as necessary*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-z2yzUxXQZ53"
      },
      "outputs": [],
      "source": [
        "# List of columns to drop\n",
        "drop_columns = [\n",
        "    'id', 'listing_url', 'scrape_id', 'last_scraped', 'source',\n",
        "    'host_url', 'host_name', 'host_since', 'host_location', 'host_about',\n",
        "    'host_thumbnail_url', 'host_picture_url', 'host_verifications', 'host_neighbourhood',\n",
        "    'host_listings_count', 'host_total_listings_count', 'host_has_profile_pic',\n",
        "    'calendar_updated', 'calendar_last_scraped', 'first_review', 'last_review',\n",
        "    'neighbourhood', 'neighborhood_overview', 'neighbourhood_group_cleansed', 'license', 'picture_url', 'host_id',\n",
        "    'maximum_nights_avg_ntm', 'has_availability', 'calendar_last_scraped',\n",
        "    'number_of_reviews_ltm', 'number_of_reviews_l30d', 'first_review', 'last_review',\n",
        "    'license', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes',\n",
        "    'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms'\n",
        "]\n",
        "\n",
        "# double check this\n",
        "# columns to consider scrutizing\n",
        "# superhost? description? (no desc, less lightly to be bought)\n",
        "\n",
        "# Drop columns\n",
        "df_cleaned = df.drop(columns=drop_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oaiugujd__p"
      },
      "outputs": [],
      "source": [
        "# this is the new cleaned dataset\n",
        "df_cleaned.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LU9UqRseeO4"
      },
      "outputs": [],
      "source": [
        "# check for remaining missing values in the columns\n",
        "df_cleaned.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd7daKrVw08W"
      },
      "outputs": [],
      "source": [
        "# Drop the rows with empty fields\n",
        "# some empty is fine, but need to consider the following : price, bedrooms, bathroom\n",
        "df_cleaned = df_cleaned.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46lOa4kq8O_O"
      },
      "outputs": [],
      "source": [
        "# check how much we are left with and ensure that there are no more field empty within the dataset\n",
        "print(df_cleaned.isnull().sum())\n",
        "print(df_cleaned.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6e2wbYLsLlW"
      },
      "outputs": [],
      "source": [
        "# view how many unique object exists in each column\n",
        "include = ['object', 'float', 'int']\n",
        "df_cleaned.describe(include=include)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sElAaXOcmUj-"
      },
      "source": [
        "Since we are predicting the price, we have to convert the data type of the price column to a readable one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSon9vzNmbpe"
      },
      "outputs": [],
      "source": [
        "# parse the price column into readable float\n",
        "df_cleaned['price'] = df_cleaned['price'].str.replace('$', '').str.replace(',', '').astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQJp9NofmjU2"
      },
      "outputs": [],
      "source": [
        "df_cleaned.price.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQmZu2Bkxkrk"
      },
      "source": [
        "**Feature Cleaning**\n",
        "\n",
        "As a huge portion of the data is categorical in nature, we can use one hot encoding (https://www.kaggle.com/code/dansbecker/using-categorical-data-with-one-hot-encoding) to cateorize if an entry has *x* or *y* property or none at all.\n",
        "\n",
        "The next sections will be dealing with feature cleaning with columns that are categorical so as to allow the model to better understand the relationship of each feature and how it correlates to the prediction of the price.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SZdQtfDgUbJ"
      },
      "source": [
        "## 1.1 Property & Physical Attributes\n",
        "\n",
        "The next few sections will be dedicated for feature engineering based on features describing the features of the physical atributes of the AirBnB listings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InVx2T07c8Vg"
      },
      "source": [
        "### 1.1.1 Cleaning of Physical Attributes Features\n",
        "\n",
        "Each features must be scrutinized and cleaned to be fitted into the model. Amenities, room types, bathroom and bedrooms and then price."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8h31s9oGGK4"
      },
      "source": [
        "**Amenities**\n",
        "\n",
        "We will first start of with amenities as an example.\n",
        "\n",
        "We need to one-hot encoding for all the amenities listed within the given amenity column.\n",
        "\n",
        "The format of the amenities is listed as an array i.e.\n",
        "\n",
        "[ \"Hangers\", \"Wi-Fi\", ... \"etc\" ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UGJ8lCvuexo"
      },
      "outputs": [],
      "source": [
        "# need to start fitting the data for a correlation map\n",
        "# get the unique values for all the amenities\n",
        "# and encode them for fitting\n",
        "import ast\n",
        "\n",
        "df_cleaned['amenities'] = df_cleaned['amenities'].apply(ast.literal_eval)\n",
        "\n",
        "# Get all unique amenities\n",
        "unique_amenities = set()\n",
        "df_cleaned['amenities'].apply(lambda x: unique_amenities.update(x))  # Flatten all amenities\n",
        "\n",
        "# Convert to sorted list for readability\n",
        "unique_amenities = sorted(unique_amenities)\n",
        "\n",
        "# Display all unique amenities\n",
        "print(unique_amenities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx6uFRQTxqhJ"
      },
      "source": [
        "The sheer number of unique amenity values means that one-hot encoding will result in too much columns for amenities, resulting in overfitting.\n",
        "\n",
        "To fix this, we need to categorise the amenities into broad categories to ensure reliable prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edGFzoKBFU1R"
      },
      "outputs": [],
      "source": [
        "# # Define categories with variations & Japanese translations\n",
        "# wifi_amenities = {\"wifi\", \"wi-fi\", \"無線lan\", \"ワイヤレスインターネット\"}\n",
        "# kitchen_amenities = {\"kitchen\", \"microwave\", \"refrigerator\", \"stove\", \"oven\", \"調理場\", \"電子レンジ\", \"冷蔵庫\", \"コンロ\", \"オーブン\"}\n",
        "# parking_amenities = {\"free parking\", \"paid parking\", \"駐車場\", \"無料駐車場\", \"有料駐車場\"}\n",
        "# pool_amenities = {\"pool\", \"swimming pool\", \"hot tub\", \"浴槽\", \"プール\", \"温泉\"}\n",
        "# air_conditioning_amenities = {\"air conditioning\", \"heating\", \"ceiling fan\", \"エアコン\", \"冷房\", \"暖房\", \"扇風機\"}\n",
        "# security_amenities = {\"smoke alarm\", \"carbon monoxide alarm\", \"first aid kit\", \"fire extinguisher\", \"防火設備\", \"火災警報器\", \"一酸化炭素警報\", \"救急キット\"}\n",
        "# laundry_amenities = {\"washer\", \"dryer\", \"laundromat\", \"iron\", \"洗濯機\", \"乾燥機\", \"ランドリー\", \"アイロン\"}\n",
        "# entertainment_amenities = {\"tv\", \"sound system\", \"streaming services\", \"テレビ\", \"スピーカー\", \"ストリーミング\"}\n",
        "\n",
        "# # Function for case-insensitive & flexible matching (check if this works ** , can convert amenti to lower caps to standardize)\n",
        "# def has_category(amenities, category):\n",
        "#     return int(any(any(keyword in amenity for keyword in category) for amenity in amenities))\n",
        "\n",
        "# # Each value will be checked according to the dictionaries, if any of string matches, place the category as 1\n",
        "# # otherwise, it will be placed as 0\n",
        "# # Apply function to dataset\n",
        "# df_cleaned['has_wifi'] = df_cleaned['amenities'].apply(lambda x: has_category(x, wifi_amenities))\n",
        "# df_cleaned['has_kitchen'] = df_cleaned['amenities'].apply(lambda x: has_category(x, kitchen_amenities))\n",
        "# df_cleaned['has_parking'] = df_cleaned['amenities'].apply(lambda x: has_category(x, parking_amenities))\n",
        "# df_cleaned['has_pool'] = df_cleaned['amenities'].apply(lambda x: has_category(x, pool_amenities))\n",
        "# df_cleaned['has_air_conditioning'] = df_cleaned['amenities'].apply(lambda x: has_category(x, air_conditioning_amenities))\n",
        "# df_cleaned['has_security'] = df_cleaned['amenities'].apply(lambda x: has_category(x, security_amenities))\n",
        "# df_cleaned['has_laundry'] = df_cleaned['amenities'].apply(lambda x: has_category(x, laundry_amenities))\n",
        "# df_cleaned['has_entertainment'] = df_cleaned['amenities'].apply(lambda x: has_category(x, entertainment_amenities))\n",
        "\n",
        "# Define categorized amenities\n",
        "wifi_amenities = {\"Wifi\", \"Wi-Fi\", \"Ethernet connection\", \"無線lan\", \"ワイヤレスインターネット\"}\n",
        "kitchen_amenities = {\"Kitchen\", \"Microwave\", \"Refrigerator\", \"Stove\", \"Oven\", \"BBQ grill\", \"Coffee maker\", \"Dining table\"}\n",
        "parking_amenities = {\"Parking\", \"Garage\", \"EV charger\", \"free parking\", \"paid parking\", \"carport\"}\n",
        "pool_amenities = {\"Pool\", \"Swimming pool\", \"Hot tub\"}\n",
        "air_conditioning_amenities = {\"Air conditioning\", \"Heating\", \"Ceiling fan\", \"Indoor Fireplace\"}\n",
        "security_amenities = {\"Smoke alarm\", \"Fire extinguisher\", \"Carbon monoxide alarm\"}\n",
        "laundry_amenities = {\"Washer\", \"Dryer\", \"Iron\", \"Clothing storage\", \"Housekeeping\"}\n",
        "bathroom_amenities = {\"Bathtub\", \"Bidet\", \"Hot water\", \"Body Soap\", \"Shampoo\", \"Conditioner\"}\n",
        "entertainment_amenities = {\"TV\", \"HDTV\", \"Sound System\", \"Game console\", \"Streaming services\"}\n",
        "fitness_amenities = {\"Exercise equipment\", \"Gym\"}\n",
        "child_friendly_amenities = {\"Baby Monitor\", \"Baby bath\", \"High chair\", \"Crib\", \"Children’s books and toys\"}\n",
        "outdoor_amenities = {\"Backyard\", \"BBQ grill\"}\n",
        "elevator_amenities = {\"Elevator\"}\n",
        "\n",
        "# Combine all categories into one dictionary\n",
        "all_amenities = {\n",
        "    \"has_wifi\": wifi_amenities,\n",
        "    \"has_kitchen\": kitchen_amenities,\n",
        "    \"has_parking\": parking_amenities,\n",
        "    \"has_pool\": pool_amenities,\n",
        "    \"has_air_conditioning\": air_conditioning_amenities,\n",
        "    \"has_security\": security_amenities,\n",
        "    \"has_laundry\": laundry_amenities,\n",
        "    \"has_bathroom\": bathroom_amenities,\n",
        "    \"has_entertainment\": entertainment_amenities,\n",
        "    \"has_fitness\": fitness_amenities,\n",
        "    \"has_child_friendly\": child_friendly_amenities,\n",
        "    \"has_outdoor\": outdoor_amenities,\n",
        "    \"has_elevator\": elevator_amenities,\n",
        "}\n",
        "\n",
        "# Convert amenity strings into lists\n",
        "df_cleaned['amenities'] = df_cleaned['amenities'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "# Apply category checks: True if any of the category amenities are in the listing\n",
        "for column, category_set in all_amenities.items():\n",
        "    df_cleaned[column] = df_cleaned['amenities'].apply(lambda x: any(item in category_set for item in x) if isinstance(x, list) else False)\n",
        "\n",
        "# Also create columns for each **individual** amenity\n",
        "for category_set in all_amenities.values():\n",
        "    for amenity in category_set:\n",
        "        col_name = f\"has_{amenity.lower().replace(' ', '_')}\"  # Normalize column name\n",
        "        df_cleaned[col_name] = df_cleaned['amenities'].apply(lambda x: amenity in x if isinstance(x, list) else False)\n",
        "\n",
        "print(df_cleaned.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhikuWbfudfP"
      },
      "source": [
        "Some of the amenity classification has 0 listings, rendering it moot, so just drop them.\n",
        "\n",
        "Should drop them if all listings have said amenity, as it would be meaningless to classify."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvwOVBPuuHDu"
      },
      "outputs": [],
      "source": [
        "# check how many listings has\n",
        "for amenity in df_cleaned.columns:\n",
        "    if amenity.startswith('has_'):\n",
        "        print(f\"{amenity}: {df_cleaned[amenity].sum()} listings have this\")\n",
        "        if (df_cleaned[amenity].sum() == 0 or df_cleaned[amenity].sum() == df_cleaned.shape[0]):\n",
        "            df_cleaned = df_cleaned.drop(columns=[amenity])\n",
        "            print(f\"Dropping {amenity}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwJ3x0h4hSp0"
      },
      "outputs": [],
      "source": [
        "df_cleaned.columns.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky7_O4ef1P4I"
      },
      "source": [
        "Feel free to add/delete/edit the amenity categories accordingly.\n",
        "\n",
        "The key idea is to single out recurring patterns to categorise them.\n",
        "\n",
        "If there is a valid use case for the categorisation i.e. splitting kitchen into *has_fridge* and *has_oven* , or *has_netflix* and/or other similar use cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qxi2mLQx0TUU"
      },
      "outputs": [],
      "source": [
        "# df.drop(columns=['amenities'], inplace = True) # Drop the amenities column if necessary\n",
        "df_cleaned.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwR3wh5R3nOV"
      },
      "source": [
        "Now we have the other unique values to process. This includes :\n",
        "\n",
        "* property_type\n",
        "* room_type\n",
        "* bathrooms\n",
        "* bathrooms_type\n",
        "* bedrooms\n",
        "* bedrooms_type\n",
        "* accomodates\n",
        "\n",
        "Other variables can be accounted for being used as factors, the aforementioned should be prioritised\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irX8FRhgz8a2"
      },
      "source": [
        "**Room Type / Property Type**\n",
        "\n",
        "Next will be the *room_type* and *property_type* of each listing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnO1shJJ7pY3"
      },
      "outputs": [],
      "source": [
        "# Each value listed is shown to be either shared or private.\n",
        "# We can also categorise the property type based on ....\n",
        "\n",
        "df_cleaned['property_type'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a23amunxb1E"
      },
      "outputs": [],
      "source": [
        "df_cleaned['room_type'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wOUqBxRVwa6"
      },
      "source": [
        "Between the 2, it better to use room_type as a better baseline comparison to avoid complicating these features.\n",
        "\n",
        "We can drop *property_type* as a result, and just one-hot encode *room_type*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQ4ICwwyWry3"
      },
      "outputs": [],
      "source": [
        "# get rid of property type\n",
        "df_cleaned = df_cleaned.drop(columns=['property_type'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dnN7PGLYNsX"
      },
      "outputs": [],
      "source": [
        "# and one-hot encode the room types\n",
        "df_cleaned = pd.get_dummies(df_cleaned, columns=['room_type'], prefix=['room_type'])  # One-hot encode room_type\n",
        "df_cleaned.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xk7QQRh7Y8E2"
      },
      "outputs": [],
      "source": [
        "df_cleaned.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts5OktViZOdA"
      },
      "source": [
        "**Bathrooms / Bathrooms Text**\n",
        "\n",
        "Next will be the bathroom features *bathrooms* and *bathroom_text*. Similar to room and property features, we are going to focus on the *bathrooms* value only as it is already standardized.\n",
        "\n",
        "The unique values will still be shown in case anyone wants to make use of the unique values in *bathroom_text*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ5R4culbeSK"
      },
      "outputs": [],
      "source": [
        "print(\"Unique numbers of bathrooms:\\n\")\n",
        "print(df_cleaned.bathrooms.unique())\n",
        "print(df_cleaned.bathrooms.value_counts())\n",
        "print(\"\\nUnique bathroom types in bathroom_text:\\n\")\n",
        "print(df_cleaned.bathrooms_text.unique())\n",
        "print(df_cleaned.bathrooms_text.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7jzbwpOb_5Z"
      },
      "outputs": [],
      "source": [
        "# drop bathroom_text for now, only normalise bathrooms if need to\n",
        "df_cleaned = df_cleaned.drop(columns=['bathrooms_text'])\n",
        "df_cleaned.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI56zIWLdwVX"
      },
      "source": [
        "**Bedrooms / Beds**\n",
        "\n",
        "Any AirBnB should have beds and bedrooms, but apparently there might be some with 0 beds and bedrooms.\n",
        "\n",
        "There's no need to clean these values at the moment, so we can leave them be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e86eI1waeE6r"
      },
      "outputs": [],
      "source": [
        "print(\"Unique count of bedrooms:\\n\")\n",
        "print(df_cleaned.bedrooms.unique())\n",
        "print(df_cleaned.bedrooms.value_counts())\n",
        "print(\"\\nUnique count of beds:\\n\")\n",
        "print(df_cleaned.beds.unique())\n",
        "print(df_cleaned.beds.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fyhN2nycjI6"
      },
      "source": [
        "Aparently, there are listings with more than 15+ bedrooms. There's even a listing with 50+ bedrooms.\n",
        "\n",
        "For beds, there are similar listings with beds over 15+ to 50?\n",
        "\n",
        "Realistically this outliers should be deleted, but feel free to keep it for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXL1LcZEdwLK"
      },
      "outputs": [],
      "source": [
        "# delete the rows with unrealistic beds and bedrooms\n",
        "df_cleaned = df_cleaned[df_cleaned['bedrooms'] <= 10]\n",
        "df_cleaned = df_cleaned[df_cleaned['beds'] <= 10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m33ImxJ3jwxv"
      },
      "source": [
        "**Accomodates**\n",
        "\n",
        "How many guests can a listing accomodate in a booking. Presumably the greater the number the greater the price.\n",
        "\n",
        "Similarly there's no need to clean it, since its a whole number with no set limit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UF55xFonjv9a"
      },
      "outputs": [],
      "source": [
        "print(df_cleaned.accommodates.unique())\n",
        "print(df_cleaned.accommodates.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZtGxZQlICCI"
      },
      "source": [
        "### 1.1.2 Price analysis\n",
        "\n",
        "Before analysing the other features, we can start with the actual prices available first.\n",
        "\n",
        "We can plot out the range of prices available, the mean of price of all legitimate listings, the highest and lowest among other comparisons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BF18G7oIssD"
      },
      "outputs": [],
      "source": [
        "print(\"Number of listing\") # Double checking\n",
        "print(df_cleaned.price.count())\n",
        "print(\"Mean price of all listings:\")\n",
        "print(df_cleaned.price.mean())\n",
        "print(\"Highest price of all listings:\")\n",
        "print(df_cleaned.price.max())\n",
        "print(\"Lowest price of all listings:\")\n",
        "print(df_cleaned.price.min())\n",
        "print(\"Median price of all listings:\")\n",
        "print(df_cleaned.price.median())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJuCIqtggdhN"
      },
      "source": [
        "There might be a necessity to remove outliers. Feel free to discuss and determine if required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClJZZ0YqMlnP"
      },
      "outputs": [],
      "source": [
        "# plot the price range in histogram for seeing price distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_cleaned['price'], bins=140, kde=True)\n",
        "plt.xlabel(\"Price (Whichever Currency being used)\")\n",
        "plt.ylabel(\"Count of Listings\")\n",
        "plt.title(\"Distribution of Listing Prices\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiuLIRdlQEj0"
      },
      "source": [
        "Due to massive outliers in the price, its hard to see the actual distribution in price, we have to use other methods to see the proper distribution.\n",
        "\n",
        "It might be good practice to remove the high percentile listings as their values might be skewing the data heavily."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tx1fywVdQDra"
      },
      "outputs": [],
      "source": [
        "# Removing high percentile outliers\n",
        "# Define a reasonable threshold (e.g., 99th percentile)\n",
        "price_threshold = np.percentile(df_cleaned[\"price\"], 95)\n",
        "\n",
        "# Filter data\n",
        "# Comment/Uncomment out the df_filtered line if you want to separate 2 datasets\n",
        "# 1 with all the listings, 1 with the top 0.5 percentile listings based onn price removed.\n",
        "# df_filtered = df_cleaned[df_cleaned[\"price\"] <= price_threshold] # Comment/Uncomment accordingly\n",
        "df_cleaned = df_cleaned[df_cleaned[\"price\"] <= price_threshold]\n",
        "print(df_cleaned.shape[0])\n",
        "# print(df_filtered.shape[0]) # Comment/Uncomment accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKprrw3GJ7jg"
      },
      "outputs": [],
      "source": [
        "# Replot\n",
        "plt.figure(figsize=(10, 6))\n",
        "# sns.histplot(df_filtered[\"price\"], bins=100, kde=True) # Comment/Uncomment accordingy\n",
        "sns.histplot(df_cleaned[\"price\"], bins=100, kde=True) # Uncomment this to run the filtered base dataset, if the the outlier was removed\n",
        "plt.xlabel(\"Price\")\n",
        "plt.ylabel(\"Count of Listings\")\n",
        "plt.title(\"Distribution of Listing Prices (Outliers Removed)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGm6Fqe2SLMS"
      },
      "outputs": [],
      "source": [
        "# Log transformation post filtered\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(np.log1p(df_cleaned[\"price\"]), bins=100, kde=True)  # log1p avoids log(0)\n",
        "plt.xlabel(\"Log(Price + 1)\")\n",
        "plt.ylabel(\"Count of Listings\")\n",
        "plt.title(\"Log-Transformed Distribution of Listing Prices\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hB1LQP2YQsk1"
      },
      "outputs": [],
      "source": [
        "# Boxplot to see outliers\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.boxplot(x=df_cleaned[\"price\"])\n",
        "plt.xlabel(\"Price\")\n",
        "plt.title(\"Boxplot of Listing Prices\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sluCeTcuRLth"
      },
      "source": [
        "As we can the majority of the distribution lies in the area between 0.0 ~ 0.2 (1000 ~ 30000). Keep in mind the price follows the le7 nototation in the boxplot if the outlier was not removed.\n",
        "\n",
        "If the outlier was not removed, then the outlier at the far right should be the listing with the $15789885.0 price. There are also 2 other outilers from 0.4 ~ 0.8 (60000~ onwards), so this should be accounted for as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm1X-Of3a-rt"
      },
      "source": [
        "For fair analysis, it might be good to remove the outliers, as it may skew the numbers greatly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rKkVX5YhqQZ"
      },
      "source": [
        "**Outliers**\n",
        "\n",
        "We are going to separate more outliers from the curve and perform analysis to see if the data is actually accurate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNJ6Z2QNh3d2"
      },
      "outputs": [],
      "source": [
        "# check here WIP\n",
        "# Compute KDE (smoothed curve from histogram)\n",
        "kde = sns.kdeplot(df_cleaned[\"price\"], bw_adjust=0.5)\n",
        "\n",
        "# Find the peak (mode) of the distribution\n",
        "peak_price = df_cleaned[\"price\"].mode()[0]\n",
        "\n",
        "# Define threshold dynamically (e.g., 3x peak price)\n",
        "threshold = 3 * peak_price\n",
        "\n",
        "# Split data\n",
        "df_cleaned_no_outliers = df_cleaned[df_cleaned[\"price\"] <= threshold]\n",
        "df_cleaned_outliers = df_cleaned[df_cleaned[\"price\"] > threshold]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJtaJzt2UB-A"
      },
      "outputs": [],
      "source": [
        "df_cleaned.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TykFKuBag185"
      },
      "source": [
        "### 1.1.3 Analysis of Physical Attributes\n",
        "\n",
        "This section we will be delving deeper into analysing each feature and its relationship to a listing's price.\n",
        "\n",
        "First, we can get a heatmap to see just how much of these features correlate to the price of each listing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlfYOB9H533R"
      },
      "source": [
        "**Amenities analysis**\n",
        "\n",
        "Keep in mind that correlation heatmaps are not entirely accurate to show the true relationship of feature to a listing's price. Amenities for example only have true/false values and not much variance; other forms of analysis may be better suited to understand which feature may have stronger weightage on price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w73Hb3E-i2bY"
      },
      "outputs": [],
      "source": [
        "# amenities only heatmap\n",
        "amenities_attribute_corr_columns = ['price']\n",
        "amenities_attribute_corr_columns.extend([col for col in df_cleaned.columns if col.startswith('has_')])\n",
        "corr_matrix = df_cleaned[amenities_attribute_corr_columns]\n",
        "# get rid of any columns that have constant values, offers nothing to correlate\n",
        "corr_matrix = corr_matrix.loc[:, corr_matrix.nunique() > 1]\n",
        "corr_matrix = corr_matrix.corr()\n",
        "\n",
        "plt.figure(figsize=(30,30))\n",
        "# sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5, annot_kws={\"size\": 8})\n",
        "\n",
        "plt.title(\"Amenity Feature Correlation Heatmap\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bClztB4Dog3R"
      },
      "outputs": [],
      "source": [
        "# Top 20 Most Correlated Features to 'Price'\n",
        "target_feature = 'price'\n",
        "top_10_corr = corr_matrix[target_feature].abs().sort_values(ascending=False)[1:21]  # Exclude self-correlation\n",
        "\n",
        "# Plot Top 20 Correlations\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top_10_corr.values, y=top_10_corr.index, palette=\"coolwarm_r\", hue=top_10_corr.index, legend=\"auto\")  # \"_r\" reverses colors\n",
        "plt.xlabel(\"Correlation\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(f\"Top 20 Features Correlated with {target_feature}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx1uHf0KW7K5"
      },
      "source": [
        "Apparently having a dinner table in your AirBnB might have the greatest positive impact on your price listing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xKhJFfDWxfd"
      },
      "source": [
        "We can do other forms of analysis with amenities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3GjDM2zW5Ll"
      },
      "outputs": [],
      "source": [
        "# Current mean of all listings without any changes\n",
        "print(\"Current price mean:\")\n",
        "print(df_cleaned.price.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyX9cgVjkhf8"
      },
      "source": [
        "We can check the mean price of all listings with any one amenitiy, and check against the mean price of all listings.\n",
        "\n",
        "By doing so, we can see if there are any sizable increase in mean price listings based if a listing has a particular amenity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ-WDH6pYQrR"
      },
      "outputs": [],
      "source": [
        "amenities_attribute_corr_columns = ['price']\n",
        "amenities_attribute_corr_columns.extend([col for col in df_cleaned.columns if col.startswith('has_')])\n",
        "check_mean = df_cleaned[amenities_attribute_corr_columns]\n",
        "\n",
        "# Create a results list\n",
        "results = []\n",
        "\n",
        "# Iterate through each amenity column (excluding 'price')\n",
        "for amenity in amenities_attribute_corr_columns[1:]:\n",
        "    mean_with = check_mean.loc[check_mean[amenity] == 1, \"price\"].mean()\n",
        "    mean_without = check_mean.loc[check_mean[amenity] == 0, \"price\"].mean()\n",
        "\n",
        "    # Append to results\n",
        "    results.append([amenity, mean_with, mean_without])\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_amenity_means = pd.DataFrame(results, columns=[\"Amenity\", \"Mean Price (With)\", \"Mean Price (Without)\"])\n",
        "\n",
        "# Display the DataFrame\n",
        "df_amenity_means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XLuY-A7loL0"
      },
      "source": [
        "We can list out the prices and compare them to see if they are above the mean or below it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWcgsYRLZ_0R"
      },
      "outputs": [],
      "source": [
        "# get rid of the columns with empty values\n",
        "df_amenity_means = df_amenity_means.dropna()\n",
        "\n",
        "# Sort by price difference\n",
        "df_amenity_means[\"Price Difference\"] = df_amenity_means[\"Mean Price (With)\"] - df_amenity_means[\"Mean Price (Without)\"]\n",
        "df_amenity_means = df_amenity_means.sort_values(\"Price Difference\", ascending=False)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(18, 10))\n",
        "sns.barplot(data=df_amenity_means, x=\"Price Difference\", y=\"Amenity\", palette=\"coolwarm_r\", legend=\"auto\", hue=\"Amenity\")\n",
        "plt.title(\"Impact of Amenities on Listing Price\")\n",
        "plt.xlabel(\"Difference in Mean Price\")\n",
        "plt.ylabel(\"Amenity\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIQRSAeIliD3"
      },
      "source": [
        "Below is a chi-squared analysis of amenity and prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Av-e_-ypkTBx"
      },
      "outputs": [],
      "source": [
        "# Chi Squared Analysis\n",
        "import scipy.stats as stats\n",
        "# amenities_attribute_corr_columns = ['price']\n",
        "# amenities_attribute_corr_columns.extend([col for col in df_cleaned.columns if col.startswith('has_')])\n",
        "# chi_sq = df_cleaned[amenities_attribute_corr_columns].copy()\n",
        "\n",
        "# # chi_sq['price_category'] = pd.qcut(chi_sq['price'], q=3, labels=[\"Low\", \"Medium\", \"High\"])\n",
        "# chi_sq['price_category'] = pd.qcut(chi_sq['price'], q=3, labels=[\"Low\", \"Medium\", \"High\"])\n",
        "# contigency_table = pd.crosstab(chi_sq['price_category'], chi_sq['has_wifi'])\n",
        "# print(contigency_table)\n",
        "\n",
        "# chi2, p, dof, expected = stats.chi2_contingency(contigency_table)\n",
        "# print(\"Chi-Squared Value:\", chi2)\n",
        "# print(\"P-Value:\", p)\n",
        "\n",
        "# # Interpretation\n",
        "# if p < 0.05:\n",
        "#     print(\"Significant relationship between has_pool and price category!\")\n",
        "# else:\n",
        "#     print(\"No significant relationship found.\")\n",
        "# Select relevant columns\n",
        "amenities_attribute_corr_columns = ['price']\n",
        "amenities_attribute_corr_columns.extend([col for col in df_cleaned.columns if col.startswith('has_')])\n",
        "chi_sq = df_cleaned[amenities_attribute_corr_columns].copy()\n",
        "\n",
        "# Create price categories\n",
        "chi_sq['price_category'] = pd.qcut(chi_sq['price'], q=3, labels=[\"Low\", \"Medium\", \"High\"])\n",
        "print(chi_sq['price_category'].value_counts())\n",
        "\n",
        "# Store results\n",
        "significant_amenities = []\n",
        "\n",
        "# Iterate over all amenities\n",
        "for amenity in [col for col in chi_sq.columns if col.startswith('has_')]:\n",
        "    contigency_table = pd.crosstab(chi_sq['price_category'], chi_sq[amenity])\n",
        "\n",
        "    # Perform Chi-Square test\n",
        "    chi2, p, dof, expected = stats.chi2_contingency(contigency_table)\n",
        "\n",
        "    # Check significance (p < 0.05 means significant relationship)\n",
        "    if p < 0.05:\n",
        "        significant_amenities.append((amenity, p))\n",
        "\n",
        "# Sort by p-value (ascending)\n",
        "significant_amenities.sort(key=lambda x: x[1])\n",
        "\n",
        "print(f\"{len(significant_amenities)} amenities have postive impact on price.\\n\")\n",
        "\n",
        "# Print results\n",
        "print(\"Amenities with a significant relationship to price category:\")\n",
        "for amenity, p_value in significant_amenities:\n",
        "    print(f\"{amenity}: p-value = {p_value:.5f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL4m5JT7XNHn"
      },
      "source": [
        "**Room types**\n",
        "\n",
        "Analysing what kind of effect the room type has on price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKdXXL9yklIJ"
      },
      "outputs": [],
      "source": [
        "# room_types only heatmap\n",
        "room_types_attribute_corr_columns = ['price', 'room_type_Entire home/apt', 'room_type_Hotel room', 'room_type_Private room', 'room_type_Shared room']\n",
        "corr_matrix = df_cleaned[room_types_attribute_corr_columns].corr()\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "\n",
        "plt.title(\"Room Type Feature Correlation Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89IXZmURl1y3"
      },
      "source": [
        "**Other physical attributes**\n",
        "\n",
        "Here we can see the heatmap and other visual representation for each feature.\n",
        "\n",
        "The graphs itself is pretty self-explanatory and simply shows the relationship between the features and price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaYxAmg0jdSy"
      },
      "outputs": [],
      "source": [
        "# the rest of the physical attribute features\n",
        "other_atritbutes_corr_columns = ['price', 'bathrooms', 'bedrooms', 'beds', 'accommodates']\n",
        "corr_matrix = df_cleaned[other_atritbutes_corr_columns].corr()\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "\n",
        "# Show plot\n",
        "plt.title(\"Other Attributes Feature Correlation Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNZ4Xv9Pf0W8"
      },
      "outputs": [],
      "source": [
        "# Price against bathrooms graph\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.lineplot(data=df_cleaned, x=\"bathrooms\", y=\"price\", errorbar=None)\n",
        "plt.xlabel(\"Number of Bathrooms Present\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Price vs. Amount of Bathrooms\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8NDnMGDbYDi"
      },
      "outputs": [],
      "source": [
        "# Price against bedrooms graph\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.lineplot(data=df_cleaned, x=\"bedrooms\", y=\"price\", errorbar=None)\n",
        "plt.xlabel(\"Number of Bedrooms\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Price vs. Bedrooms\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvg2IDrJfYqr"
      },
      "outputs": [],
      "source": [
        "# Price against beds graph\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.lineplot(data=df_cleaned, x=\"beds\", y=\"price\", errorbar=None)\n",
        "plt.xlabel(\"Number of Beds\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Price vs. Beds\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ud4oqizRgRQz"
      },
      "outputs": [],
      "source": [
        "# Price against accomodation graph\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.lineplot(data=df_cleaned, x=\"accommodates\", y=\"price\", errorbar=None)\n",
        "plt.xlabel(\"Number of Guests Accommodated\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Price vs. Guest Capacity\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy4ar_kBmPt-"
      },
      "source": [
        "Currently due to outliers in the dataset, some of the graphs look pretty inaccurate.\n",
        "\n",
        "Will have to discuss if want to remove them in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bptG7MJMpgYy"
      },
      "source": [
        "## 1.2 Location Attributes\n",
        "\n",
        "Unknown if want to proceed with this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "location_attributes=['price','longitude','latitude','neighbourhood_cleansed']\n",
        "df_cleaned[location_attributes].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYav0Ac0UL8w"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=df_cleaned, x=\"longitude\", y=\"latitude\", hue=\"price\", palette=\"coolwarm\", alpha=0.7)\n",
        "plt.xlabel(\"Longitude\")\n",
        "plt.ylabel(\"Latitude\")\n",
        "plt.title(\"Price Distribution by Location\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFsXrZDQpzAw"
      },
      "source": [
        "## 1.3 Host Atttributes\n",
        "\n",
        "Feature Engineering with features corresponding to host information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KyhPDG8yG3S"
      },
      "source": [
        "The list of Host attribute features consists of:\n",
        "\n",
        " * 'host_response_time'\n",
        " * 'host_response_rate'\n",
        " * 'host_acceptance_rate'\n",
        " * 'host_is_superhost'\n",
        " * 'host_identity_verified'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RH0beGJaymGJ"
      },
      "outputs": [],
      "source": [
        "host_attributes = ['price','host_response_time','host_response_rate','host_acceptance_rate','host_is_superhost','host_identity_verified']\n",
        "df_cleaned[host_attributes].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FO6jDUWvL-Z"
      },
      "source": [
        "**Cleaning Features**\n",
        "\n",
        "We have to clean the data once again and see the relationship between these features and price."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6DDPCJXNtda"
      },
      "source": [
        "**Response Time and Rate, Acceptance Rate**\n",
        "\n",
        "These features are unique and have to be normalised/tagged before they are able to be used for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW9ARacwKiFO"
      },
      "outputs": [],
      "source": [
        "# pull the values and see unique counts, whats available\n",
        "# start with response time and rate, with acceptance rate\n",
        "print(\"Response time data:\\n\")\n",
        "print(df_cleaned.host_response_time.unique())\n",
        "print(df_cleaned.host_response_time.value_counts())\n",
        "print(\"\\nResponse rate data:\\n\")\n",
        "print(df_cleaned.host_response_rate.unique())\n",
        "print(df_cleaned.host_response_rate.value_counts())\n",
        "print(\"\\nAcceptance rate data:\\n\")\n",
        "print(df_cleaned.host_acceptance_rate.unique())\n",
        "print(df_cleaned.host_acceptance_rate.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRO_HlHEQuEk"
      },
      "source": [
        "We can encode the values for the response time, and convert the percentages into numerical values and normalise them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elmSnx5mQ3pJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "unique_response_time = df_cleaned.host_response_time.unique()\n",
        "mapped = le.fit_transform(unique_response_time)\n",
        "\n",
        "mapping = dict(zip(mapped, unique_response_time))\n",
        "sorted_dict = dict(sorted(mapping.items()))\n",
        "print(sorted_dict)\n",
        "\n",
        "df_cleaned['host_response_time'] = le.fit_transform(df_cleaned['host_response_time'])\n",
        "df_cleaned_no_outliers['host_response_time'] = le.fit_transform(df_cleaned_no_outliers['host_response_time'])\n",
        "df_cleaned['host_response_time'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnPz08XwSu4L"
      },
      "outputs": [],
      "source": [
        "# Convert percentage strings to float and normalize\n",
        "df_cleaned[\"host_response_rate\"] = df_cleaned[\"host_response_rate\"].str.rstrip(\"%\").astype(float) / 100\n",
        "df_cleaned[\"host_acceptance_rate\"] = df_cleaned[\"host_acceptance_rate\"].str.rstrip(\"%\").astype(float) / 100\n",
        "\n",
        "# Convert percentage strings to float and normalize\n",
        "df_cleaned_no_outliers[\"host_response_rate\"] = df_cleaned_no_outliers[\"host_response_rate\"].str.rstrip(\"%\").astype(float) / 100\n",
        "df_cleaned_no_outliers[\"host_acceptance_rate\"] = df_cleaned_no_outliers[\"host_acceptance_rate\"].str.rstrip(\"%\").astype(float) / 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoRLaFZzVM1T"
      },
      "outputs": [],
      "source": [
        "# show the new cleaned values\n",
        "df_cleaned[[\"host_response_rate\", \"host_acceptance_rate\"]].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0GB88sXLAdK"
      },
      "outputs": [],
      "source": [
        "# For superhost and verified columns\n",
        "print(df_cleaned.host_is_superhost.value_counts())\n",
        "print(f\"Percentage of hosts that are superhost: {(df_cleaned.host_is_superhost.value_counts().iloc[0]/df_cleaned.host_is_superhost.value_counts().sum())*100:.2f}\")\n",
        "print(df_cleaned.host_identity_verified.value_counts())\n",
        "print(f\"Percentage of hosts that are verified: {(df_cleaned.host_identity_verified.value_counts().iloc[0]/df_cleaned.host_identity_verified.value_counts().sum())*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HD7HgdyNrM2"
      },
      "outputs": [],
      "source": [
        "# one hot encode the superhost and verified columns\n",
        "df_cleaned[\"host_is_superhost\"] = df_cleaned[\"host_is_superhost\"].map({\"t\": True, \"f\": False})\n",
        "df_cleaned[\"host_identity_verified\"] = df_cleaned[\"host_identity_verified\"].map({\"t\": True, \"f\": False})\n",
        "df_cleaned_no_outliers[\"host_is_superhost\"] = df_cleaned_no_outliers[\"host_is_superhost\"].map({\"t\": True, \"f\": False})\n",
        "df_cleaned_no_outliers[\"host_identity_verified\"] = df_cleaned_no_outliers[\"host_identity_verified\"].map({\"t\": True, \"f\": False})\n",
        "\n",
        "df_cleaned[[\"host_is_superhost\", \"host_identity_verified\"]].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HttHBVy1ZbSf"
      },
      "source": [
        "**Analysis**\n",
        "\n",
        "Now we can further analyse the features and their relationship to price.\n",
        "\n",
        "As usual we can start with the heatmap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWh-85f5ZppY"
      },
      "outputs": [],
      "source": [
        "# generate heatmap\n",
        "corr_host = df_cleaned[host_attributes].corr()\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.heatmap(corr_host, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "\n",
        "# Show plot\n",
        "plt.title(\"Host Features Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2nx9OdFanKL"
      },
      "source": [
        "Lack of variance in data (i.e. actual numerical value) makes it hard to see positive relationships in heatmaps\n",
        "\n",
        "It would be better to see other forms of analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO7oYXVcbP2W"
      },
      "source": [
        "**Response Time**\n",
        "\n",
        "Deos response time seem to affect pricing? It seems to be somewhat affected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUFuEhKZaz35"
      },
      "outputs": [],
      "source": [
        "# Host response time vs. Price\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(x=\"host_response_time\", y=\"price\", data=df_cleaned)\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Price Distribution by Host Response Time\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDhJWwYJb6O0"
      },
      "source": [
        "Do higher response/acceptance rates lead to higher/lower prices?\n",
        "Are there clusters of hosts charging extreme prices with low engagement?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdDpz060bNZG"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(x=\"host_response_rate\", y=\"price\", data=df_cleaned, alpha=0.5)\n",
        "plt.title(\"Price vs. Host Response Rate\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oa5uPjT8cGc2"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(x=\"host_acceptance_rate\", y=\"price\", data=df_cleaned, alpha=0.5)\n",
        "plt.title(\"Price vs. Host Acceptance Rate\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8GOZEKtcE0z"
      },
      "outputs": [],
      "source": [
        "print(df_cleaned[\"price\"].mean())\n",
        "print(df_cleaned.groupby(\"host_response_time\")[\"price\"].mean().sort_values())\n",
        "print(df_cleaned.groupby(\"host_is_superhost\")[\"price\"].mean())\n",
        "\n",
        "# plot into bar graph\n",
        "# Calculate overall average price\n",
        "avg_price = df_cleaned[\"price\"].mean()\n",
        "\n",
        "# Create bar plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "ax = sns.barplot(x=\"host_response_time\", y=\"price\", data=df_cleaned, estimator=lambda x: x.mean())\n",
        "\n",
        "# Add a horizontal line for average price\n",
        "plt.axhline(avg_price, color='red', linestyle='dashed', linewidth=2, label=f'Avg Price: {avg_price:.2f}')\n",
        "\n",
        "# Annotate each bar with its mean price\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height():.2f}',\n",
        "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='bottom', fontsize=10, color='black')\n",
        "\n",
        "# Add title and legend\n",
        "plt.title(\"Average Price by Host Response Time\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3-4y9Qrb7d7"
      },
      "source": [
        "Are Superhosts charging more on average?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "On9YJxNReYq3"
      },
      "outputs": [],
      "source": [
        "# Host is superhost vs. Price\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.boxplot(x=\"host_is_superhost\", y=\"price\", data=df_cleaned)\n",
        "plt.title(\"Price Distribution for Superhosts vs. Non-Superhosts\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhhAleKEcjwZ"
      },
      "outputs": [],
      "source": [
        "# Create bar plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "ax = sns.barplot(x=\"host_is_superhost\", y=\"price\", data=df_cleaned, estimator=lambda x: x.mean())\n",
        "\n",
        "# Add a horizontal line for average price\n",
        "plt.axhline(avg_price, color='red', linestyle='dashed', linewidth=2, label=f'Avg Price: {avg_price:.2f}')\n",
        "\n",
        "# Annotate each bar with its mean price\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height():.2f}',\n",
        "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='bottom', fontsize=10, color='black')\n",
        "\n",
        "# Set proper xticks before setting labels\n",
        "ax.set_xticks([0, 1])\n",
        "ax.set_xticklabels([\"Non-Superhost\", \"Superhost\"])\n",
        "\n",
        "# Add title and legend\n",
        "plt.title(\"Average Price by Superhost Status\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wrh3csjkdrpY"
      },
      "outputs": [],
      "source": [
        "# Create bar plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "ax = sns.barplot(x=\"host_identity_verified\", y=\"price\", data=df_cleaned, estimator=lambda x: x.mean())\n",
        "\n",
        "# Add a horizontal line for average price\n",
        "plt.axhline(avg_price, color='red', linestyle='dashed', linewidth=2, label=f'Avg Price: {avg_price:.2f}')\n",
        "\n",
        "# Annotate each bar with its mean price\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height():.2f}',\n",
        "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='bottom', fontsize=10, color='black')\n",
        "\n",
        "# Set proper xticks before setting labels\n",
        "ax.set_xticks([0, 1])\n",
        "ax.set_xticklabels([\"Non-Verified\", \"Verified\"])\n",
        "\n",
        "# Add title and legend\n",
        "plt.title(\"Average Price by Verified Status\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWsV2ITfqhda"
      },
      "source": [
        "## 1.4 Review Attributes\n",
        "\n",
        "Feature Engineering with features corresponding to reviews given by users for each listing.\n",
        "\n",
        "This conists of :\n",
        "\n",
        "* 'number_of_reviews',\n",
        "* 'review_scores_rating',\n",
        "* 'review_scores_accuracy',\n",
        "* 'review_scores_cleanliness',\n",
        "* 'review_scores_checkin',\n",
        "* 'review_scores_communication',\n",
        "* 'review_scores_location',\n",
        "* 'review_scores_value',\n",
        "* 'reviews_per_month',"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljYnvvBW0Tfg"
      },
      "outputs": [],
      "source": [
        "review_attributes = ['price',\n",
        "'number_of_reviews',\n",
        "'review_scores_rating',\n",
        "'review_scores_accuracy',\n",
        "'review_scores_cleanliness',\n",
        "'review_scores_checkin',\n",
        "'review_scores_communication',\n",
        "'review_scores_location',\n",
        "'review_scores_value',\n",
        "'reviews_per_month']\n",
        "df_cleaned[review_attributes].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJ-rbr56fpi-"
      },
      "outputs": [],
      "source": [
        "# for number of reviews, check the mean and outliers\n",
        "print(df_cleaned.number_of_reviews.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IjZaVighRmi"
      },
      "outputs": [],
      "source": [
        "# normalise the review scores\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Select only the review score columns\n",
        "review_cols = [\n",
        "    \"review_scores_rating\", \"review_scores_accuracy\", \"review_scores_cleanliness\",\n",
        "    \"review_scores_checkin\", \"review_scores_communication\", \"review_scores_location\", \"review_scores_value\"\n",
        "]\n",
        "\n",
        "# Apply MinMaxScaler (0-5 → 0-1)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "df_cleaned[review_cols] = scaler.fit_transform(df_cleaned[review_cols])\n",
        "df_cleaned[review_attributes].head()\n",
        "df_cleaned_no_outliers[review_cols] = scaler.fit_transform(df_cleaned_no_outliers[review_cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iF01i7whXk_"
      },
      "outputs": [],
      "source": [
        "# check the monthly reviews\n",
        "print(df_cleaned.reviews_per_month.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMBU4BQFh_xd"
      },
      "outputs": [],
      "source": [
        "# heatmap\n",
        "corr_review = df_cleaned[review_attributes].corr()\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.heatmap(corr_review, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ar0YMve0iL9Y"
      },
      "outputs": [],
      "source": [
        "# prices against number of reviews\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.lineplot(data=df_cleaned, x=\"number_of_reviews\", y=\"price\", errorbar=None)\n",
        "plt.xlabel(\"Number of Reviews per Listing\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Price vs. Reviews\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iW3BT1ZmUPL9"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(x=\"review_scores_rating\", y=\"price\", data=df_cleaned, alpha=0.5)\n",
        "plt.title(\"Price vs. Review Score Rating Distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L1K_wLfVAyF"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(x=\"review_scores_cleanliness\", y=\"price\", data=df_cleaned, alpha=0.5)\n",
        "plt.title(\"Price vs. Reviews Scores Cleanliness Distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_caqVg_nVdxS"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(x=\"reviews_per_month\", y=\"price\", data=df_cleaned, alpha=0.5)\n",
        "plt.title(\"Price vs. Reviews Per Month Distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUPwTfxCV5ON"
      },
      "source": [
        "There's not much correlation in this dataset to warrent using the reviews statistics as a main feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDLX9YNCzTrN"
      },
      "source": [
        "## 1.5 AirBnB Booking Attributes\n",
        "\n",
        "This is feature engineering on features related to other factors related to the booking process.\n",
        "\n",
        "This includes:\n",
        "\n",
        "* 'minimum_nights',\n",
        "* 'maximum_nights',\n",
        "* 'minimum_minimum_nights',\n",
        "* 'maximum_minimum_nights',\n",
        "* 'minimum_maximum_nights',\n",
        "* 'maximum_maximum_nights',\n",
        "* 'minimum_nights_avg_ntm',\n",
        "* 'availability_30',\n",
        "* 'availability_60',\n",
        "* 'availability_90',\n",
        "* 'availability_365',\n",
        "* 'instant_bookable',"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e3wLnQH0ezp"
      },
      "outputs": [],
      "source": [
        "booking_attributes = ['minimum_nights',\n",
        "'maximum_nights',\n",
        "'minimum_minimum_nights',\n",
        "'maximum_minimum_nights',\n",
        "'minimum_maximum_nights',\n",
        "'maximum_maximum_nights',\n",
        "'minimum_nights_avg_ntm',\n",
        "'availability_30',\n",
        "'availability_60',\n",
        "'availability_90',\n",
        "'availability_365',\n",
        "'instant_bookable']\n",
        "df_cleaned[booking_attributes].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ey1bAZn1iUsv"
      },
      "outputs": [],
      "source": [
        "# can consider dropping some of the columns here due to redundancy\n",
        "print(df_cleaned[\"minimum_nights\"].unique())\n",
        "print(df_cleaned[\"minimum_nights\"].value_counts())\n",
        "print(df_cleaned[\"maximum_nights\"].unique())\n",
        "print(df_cleaned[\"maximum_nights\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-B58rINPaK63"
      },
      "outputs": [],
      "source": [
        "# check the availabilty columns\n",
        "print(df_cleaned['availability_30'].unique())\n",
        "print(df_cleaned['availability_30'].value_counts())\n",
        "print(df_cleaned['availability_60'].unique())\n",
        "print(df_cleaned['availability_60'].value_counts())\n",
        "print(df_cleaned['availability_90'].unique())\n",
        "print(df_cleaned['availability_90'].value_counts())\n",
        "print(df_cleaned['availability_365'].unique())\n",
        "print(df_cleaned['availability_365'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSXQoz04WmcX"
      },
      "outputs": [],
      "source": [
        "# can drop the min-min nights, max-min nights, min-max nights, max-max nights, kinda redundant\n",
        "# print(len(df_cleaned.columns))\n",
        "# df_cleaned.drop(columns=[\"minimum_minimum_nights\", \"maximum_minimum_nights\", \"minimum_maximum_nights\", \"maximum_maximum_nights\"], inplace=True)\n",
        "# df_cleaned_no_outliers.drop(columns=[\"minimum_minimum_nights\", \"maximum_minimum_nights\", \"minimum_maximum_nights\", \"maximum_maximum_nights\"], inplace=True)\n",
        "# print(len(df_cleaned.columns))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L98UAZO4Xvuz"
      },
      "source": [
        "Leave availabilty as it is. Leave the average nights as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRdi9zffXoKH"
      },
      "outputs": [],
      "source": [
        "# map instant bookable\n",
        "df_cleaned[\"instant_bookable\"] = df_cleaned[\"instant_bookable\"].map({\"t\": True, \"f\": False})\n",
        "df_cleaned_no_outliers[\"instant_bookable\"] = df_cleaned_no_outliers[\"instant_bookable\"].map({\"t\": True, \"f\": False})\n",
        "df_cleaned[\"instant_bookable\"].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZabrxufXYaCp"
      },
      "outputs": [],
      "source": [
        "# heatmap the shit\n",
        "corr_columns = ['price',\n",
        "'minimum_nights',\n",
        "'maximum_nights',\n",
        "'minimum_nights_avg_ntm',\n",
        "'availability_30',\n",
        "'availability_60',\n",
        "'availability_90',\n",
        "'availability_365',\n",
        "'instant_bookable']\n",
        "\n",
        "corr_host = df_cleaned[corr_columns].corr()\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.heatmap(corr_host, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "\n",
        "# Show plot\n",
        "plt.title(\"Booking Features Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOdd4TTAcsmK"
      },
      "outputs": [],
      "source": [
        "# Check minimum nights graph against price\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.scatterplot(x=df_cleaned[\"minimum_nights\"], y=df_cleaned[\"price\"])\n",
        "plt.title(\"Price vs. Minimum Nights\")\n",
        "plt.xlabel(\"Minimum Nights\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfzn2dPgZW2t"
      },
      "source": [
        "The booking attributes seem to have poor correlated attributes. For now, only the features *avaliabilty_365* and *instant_bookable* seem to be worth looking into."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3i_5_2BbWfF"
      },
      "outputs": [],
      "source": [
        "# analyse availabilty_365\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.scatterplot(x=df_cleaned[\"availability_365\"], y=df_cleaned[\"price\"])\n",
        "plt.title(\"Price vs. Availability (Days per Year)\")\n",
        "plt.xlabel(\"Availability (Days in a Year)\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dnmof7QUcnNx"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "sns.boxplot(x=df_cleaned[\"host_is_superhost\"], y=df_cleaned[\"availability_365\"])\n",
        "plt.title(\"Availability by Superhost Status\")\n",
        "plt.xlabel(\"Superhost\")\n",
        "plt.ylabel(\"Availability (Days per Year)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYWIzhhYcnQr"
      },
      "outputs": [],
      "source": [
        "df_viz = df_cleaned.copy()  # Create a copy to avoid modifying the original data\n",
        "room_col = ['room_type_Entire home/apt','room_type_Hotel room',\t'room_type_Private room',\t'room_type_Shared room']\n",
        "\n",
        "# Convert one-hot encoded columns back to a single categorical column\n",
        "df_viz[\"room_type\"] = df_viz[room_col].idxmax(axis=1).str.replace(\"room_type_\", \"\")\n",
        "\n",
        "# Now plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(x=df_viz[\"room_type\"], y=df_viz[\"availability_365\"])\n",
        "plt.title(\"Availability by Room Type\")\n",
        "plt.xlabel(\"Room Type\")\n",
        "plt.ylabel(\"Availability (Days per Year)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RyQkcute2mm"
      },
      "outputs": [],
      "source": [
        "# See distribution of availabilty\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df_cleaned[\"availability_365\"], bins=30, kde=True)\n",
        "plt.title(\"Distribution of Availability (Days per Year)\")\n",
        "plt.xlabel(\"Availability (Days in a Year)\")\n",
        "plt.ylabel(\"Count of Listings\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DRzkDklchB7"
      },
      "source": [
        "It is hard to see any kind of relationship here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bri6tJd2aD5c"
      },
      "outputs": [],
      "source": [
        "# Create bar plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "ax = sns.barplot(x=\"instant_bookable\", y=\"price\", data=df_cleaned, estimator=lambda x: x.mean())\n",
        "\n",
        "# Add a horizontal line for average price\n",
        "plt.axhline(avg_price, color='red', linestyle='dashed', linewidth=2, label=f'Avg Price: {avg_price:.2f}')\n",
        "\n",
        "# Annotate each bar with its mean price\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height():.2f}',\n",
        "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='bottom', fontsize=10, color='black')\n",
        "\n",
        "# Set proper xticks before setting labels\n",
        "ax.set_xticks([0, 1])\n",
        "ax.set_xticklabels([\"Not Instant Bookable\", \"Is Instant Bookable\"])\n",
        "\n",
        "# Add title and legend\n",
        "plt.title(\"Average Price by Booking Status\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfeEvy1rfzbJ"
      },
      "source": [
        "## 1.6 Final Findings\n",
        "\n",
        "Use most of the property/physical attributes, Host attributes maybe, reviews are not so good, Booking attributes only use some. fuck the location anyone want do that have fun lazy\n",
        "\n",
        "WIP, do later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PL5SzJagfObL"
      },
      "outputs": [],
      "source": [
        "print(df_cleaned.columns)\n",
        "# drop the following\n",
        "drop_list = ['minimum_nights',\n",
        "'maximum_nights',\n",
        "'minimum_minimum_nights',\n",
        "'maximum_minimum_nights',\n",
        "'minimum_maximum_nights',\n",
        "'maximum_maximum_nights',\n",
        "'minimum_nights_avg_ntm',\n",
        "'availability_30',\n",
        "'availability_60',\n",
        "'availability_90',\n",
        "'availability_365',\n",
        "'name',\n",
        "'description',\n",
        "'review_scores_accuracy',\n",
        "'review_scores_cleanliness',\n",
        "'review_scores_checkin',\n",
        "'review_scores_communication',\n",
        "'review_scores_location',\n",
        "'review_scores_value',\n",
        "'host_identity_verified',\n",
        "'amenities',\n",
        "'neighbourhood_cleansed',\n",
        "'longitude',\n",
        "'latitude']\n",
        "df_cleaned = df_cleaned.drop(columns=drop_list)\n",
        "df_cleaned_no_outliers = df_cleaned_no_outliers.drop(columns=drop_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7N44EisgWaL"
      },
      "outputs": [],
      "source": [
        "print(df_cleaned.columns)\n",
        "print(len(df_cleaned.columns))\n",
        "df_cleaned.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WX7nCHN0GVJq"
      },
      "source": [
        "# 2. Data Fitting\n",
        "\n",
        "Place the relevant factors to be used for training and testing model\n",
        "\n",
        "To be done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KZ8HpG891ef"
      },
      "outputs": [],
      "source": [
        "# Import list\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLhyhEz-E8x8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Ensure no division by zero (adding +1)\n",
        "df_cleaned[\"room_density\"] = df_cleaned[\"beds\"] / (df_cleaned[\"bedrooms\"] + 1)\n",
        "df_cleaned[\"space_per_bed\"] = df_cleaned[\"accommodates\"] / (df_cleaned[\"beds\"] + 1)\n",
        "\n",
        "# Log Transform (if needed)\n",
        "df_cleaned[\"log_accommodation\"] = np.log1p(df_cleaned[\"accommodates\"])\n",
        "df_cleaned[\"log_bedrooms\"] = np.log1p(df_cleaned[\"bedrooms\"])\n",
        "df_cleaned[\"log_beds\"] = np.log1p(df_cleaned[\"beds\"])\n",
        "\n",
        "# Polynomial Features\n",
        "df_cleaned[\"bedrooms_squared\"] = df_cleaned[\"bedrooms\"] ** 2\n",
        "df_cleaned[\"beds_squared\"] = df_cleaned[\"beds\"] ** 2\n",
        "df_cleaned[\"accommodation_beds_interaction\"] = df_cleaned[\"accommodates\"] * df_cleaned[\"beds\"]\n",
        "\n",
        "# K-Means Clustering (Choose a reasonable number of clusters, e.g., 4)\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "df_cleaned[\"room_cluster\"] = kmeans.fit_predict(df_cleaned[[\"accommodates\", \"bedrooms\", \"beds\"]])\n",
        "\n",
        "engineered_cols = ['price',\n",
        "                   'room_density',\n",
        "                   'space_per_bed',\n",
        "                   'log_accommodation',\n",
        "                   'log_bedrooms',\n",
        "                   'log_beds',\n",
        "                   'bedrooms_squared',\n",
        "                   'beds_squared',\n",
        "                   'accommodation_beds_interaction',\n",
        "                   'room_cluster']\n",
        "\n",
        "# Display the first few rows to check the new features\n",
        "df_cleaned[engineered_cols].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hYYyTWfE9zI"
      },
      "outputs": [],
      "source": [
        "# Compute correlation with target variable (e.g., 'price')\n",
        "corr_matrix = df_cleaned[engineered_cols].corr()\n",
        "\n",
        "# Sort correlations to see the strongest ones\n",
        "corr_with_target = corr_matrix[\"price\"].sort_values(ascending=False)\n",
        "print(corr_with_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfsL4S3nH-KZ"
      },
      "outputs": [],
      "source": [
        "df_cleaned.drop(columns=['room_cluster','space_per_bed', 'room_density'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dM2hkNkMQDcs"
      },
      "outputs": [],
      "source": [
        "df_cleaned.columns.to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glydNrgMGYlO"
      },
      "source": [
        "# 3. Model Training\n",
        "\n",
        "To be done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIkHPbsI2nYN"
      },
      "source": [
        "## **SVM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WI5mDT3ZW99b"
      },
      "outputs": [],
      "source": [
        "#SVM testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mrd6oFY1X5Ah"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Drop 'price' column from features\n",
        "X = df_cleaned.drop(columns=['price'])\n",
        "y = df_cleaned['price']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtsPDwPIJrBC"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import learning_curve\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VoPKZkqJZa4"
      },
      "outputs": [],
      "source": [
        "# SVM Linear\n",
        "X = df_cleaned.drop(columns=['price'])\n",
        "y = df_cleaned['price']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "result = []\n",
        "svm_linear = SVR(kernel='linear')\n",
        "svm_linear.fit(X_train,y_train)\n",
        "\n",
        "y_pred = svm_linear.predict(X_test)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "results.append(('linear', mae, mse, rmse, r2))\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.scatterplot(x=y_test,y=y_pred, alpha=0.6)\n",
        "plt.xlabel(\"Actual Prices\")\n",
        "plt.ylabel(\"Predicted Prices\")\n",
        "plt.title(\"Actual vs. Predicted Prices (SVM - Linear Kernel)\")\n",
        "plt.show()\n",
        "\n",
        "# Compute learning curve\n",
        "train_sizes, train_scores, val_scores = learning_curve(\n",
        "    svm_linear, X_train, y_train, cv=5, scoring=\"neg_mean_absolute_error\",\n",
        "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
        ")\n",
        "\n",
        "# Calculate mean & std deviation\n",
        "train_mean = -np.mean(train_scores, axis=1)\n",
        "train_std = np.std(train_scores, axis=1)\n",
        "val_mean = -np.mean(val_scores, axis=1)\n",
        "val_std = np.std(val_scores, axis=1)\n",
        "\n",
        "# Plot learning curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(train_sizes, train_mean, label=\"Training Error\", color=\"blue\")\n",
        "plt.fill_between(train_sizes, train_mean-train_std, train_mean+train_std, alpha=0.2, color=\"blue\")\n",
        "plt.plot(train_sizes, val_mean, label=\"Validation Error\", color=\"red\")\n",
        "plt.fill_between(train_sizes, val_mean-val_std, val_mean+val_std, alpha=0.2, color=\"red\")\n",
        "\n",
        "plt.xlabel(\"Training Set Size\")\n",
        "plt.ylabel(\"MAE (Lower is Better)\")\n",
        "plt.title(\"Learning Curve (SVM - Linear Kernel)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auigv2CAOEj4"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "num_features = [\n",
        "    'host_response_rate', 'host_acceptance_rate', 'accommodates', 'bathrooms',\n",
        "    'bedrooms', 'beds', 'number_of_reviews', 'review_scores_rating',\n",
        "    'reviews_per_month', 'log_accommodation', 'accommodation_beds_interaction',\n",
        "    'log_beds', 'log_bedrooms', 'beds_squared', 'bedrooms_squared',\n",
        "    'room_density', 'space_per_bed'\n",
        "]\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), num_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "svm_pipeline = Pipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"svm\", SVR(kernel=\"linear\"))\n",
        "])\n",
        "\n",
        "# Create separate scaler for y (target)\n",
        "y_scaler = StandardScaler()\n",
        "\n",
        "# Scale y (target)\n",
        "# y_train_scaled = y_scaler.fit_transform(y_train)\n",
        "# y_test_scaled = y_scaler.transform(y_test)\n",
        "# Scale y (target)\n",
        "y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_scaled = y_scaler.transform(y_test.values.reshape(-1, 1))  # Only transform, don't fit\n",
        "\n",
        "\n",
        "svm_pipeline.fit(X_train,y_train_scaled.ravel())\n",
        "y_pred_scaled = svm_pipeline.predict(X_test)\n",
        "\n",
        "# Inverse transform predictions to get back original price scale\n",
        "y_pred = y_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1))\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "results.append(('scaled_linear', mae, mse, rmse, r2))\\\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "# sns.scatterplot(x=y_test,y=y_pred, alpha=0.6)\n",
        "sns.scatterplot(x=y_test, y=y_pred.ravel(), alpha=0.6)\n",
        "plt.xlabel(\"Actual Prices\")\n",
        "plt.ylabel(\"Predicted Prices\")\n",
        "plt.title(\"Actual vs. Predicted Prices (SVM - Scaled Linear Kernel)\")\n",
        "plt.show()\n",
        "\n",
        "# Compute learning curve\n",
        "train_sizes, train_scores, val_scores = learning_curve(\n",
        "    svm_pipeline, X_train, y_train, cv=5, scoring=\"neg_mean_absolute_error\",\n",
        "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
        ")\n",
        "\n",
        "# Calculate mean & std deviation\n",
        "train_mean = -np.mean(train_scores, axis=1)\n",
        "train_std = np.std(train_scores, axis=1)\n",
        "val_mean = -np.mean(val_scores, axis=1)\n",
        "val_std = np.std(val_scores, axis=1)\n",
        "\n",
        "# Plot learning curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(train_sizes, train_mean, label=\"Training Error\", color=\"blue\")\n",
        "plt.fill_between(train_sizes, train_mean-train_std, train_mean+train_std, alpha=0.2, color=\"blue\")\n",
        "plt.plot(train_sizes, val_mean, label=\"Validation Error\", color=\"red\")\n",
        "plt.fill_between(train_sizes, val_mean-val_std, val_mean+val_std, alpha=0.2, color=\"red\")\n",
        "\n",
        "plt.xlabel(\"Training Set Size\")\n",
        "plt.ylabel(\"MAE (Lower is Better)\")\n",
        "plt.title(\"Learning Curve (SVM - Scaled Linear Kernel)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lRLvQZFNDrR"
      },
      "outputs": [],
      "source": [
        "print(results[-2])\n",
        "print(results[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdjTltuvrQV2"
      },
      "source": [
        "**Cross-Validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7p9tpdaqcpn"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "cv_scores = cross_val_score(svm_pipeline, X_train, y_train_scaled.ravel(), cv=5, scoring=\"neg_mean_absolute_error\")\n",
        "\n",
        "# Convert scores to positive MAE values\n",
        "cv_mae_scores = -cv_scores\n",
        "\n",
        "print(f\"Cross-Validation MAE Scores: {cv_mae_scores}\")\n",
        "print(f\"Mean MAE: {cv_mae_scores.mean():.4f}, Std Dev: {cv_mae_scores.std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKHUGM2Yrd4G"
      },
      "source": [
        "**Hyperparameter**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0wWfwdxkuqH",
        "outputId": "2cbad75c-c561-4f2d-89e1-8fade0dae6fc"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform\n",
        "\n",
        "param_dist = {\n",
        "    'C': [0.1, 10],  # Reduced from [0.1, 1, 10, 100]\n",
        "    'epsilon': [0.1, 1, 10],  # Reduced from [0.01, 0.1, 1, 5, 10]\n",
        "    'kernel': ['linear', 'rbf'],  # Keep the best two\n",
        "    'gamma': ['scale']  # Remove 'auto' (scale is better for most cases)\n",
        "}\n",
        "\n",
        "# Randomized search with cross-validation\n",
        "random_search = RandomizedSearchCV(\n",
        "    svm_pipeline,\n",
        "    param_dist,\n",
        "    n_iter=15,\n",
        "    cv=5,\n",
        "    scoring='neg_mean_absolute_error',\n",
        "    verbose=1,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "random_search.fit(X_train, y_train_scaled.ravel())\n",
        "\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Best MAE:\", -random_search.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "best_svr_random_search = svm_pipeline(**random_search.best_params_)\n",
        "best_svr_random_search.fit(X_train, y_train_scaled.ravel())\n",
        "\n",
        "y_pred_random_search = best_svr_random_search.predict(X_test) \n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred_random_search)\n",
        "mse = mean_squared_error(y_test, y_pred_random_search)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred_random_search)\n",
        "\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"MSE: {mse:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"R²: {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bayesian parameter tuning\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real\n",
        "\n",
        "# Define search space\n",
        "param_space = {\n",
        "    'C': Real(0.1, 100, prior='log-uniform'),  # Log-uniform helps explore a wide range\n",
        "    'epsilon': Real(0.01, 1, prior='log-uniform')  # Small to moderate epsilon values\n",
        "}\n",
        "\n",
        "# Define the Bayesian optimizer\n",
        "bayes_search = BayesSearchCV(\n",
        "    estimator=SVR(kernel='linear'),  # Only using linear kernel\n",
        "    search_spaces=param_space,\n",
        "    n_iter=50,  # Number of iterations\n",
        "    cv=5,  # Cross-validation folds\n",
        "    n_jobs=-1,  # Use all available cores\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "bayes_search.fit(X_train, y_train_scaled.ravel())\n",
        "\n",
        "# Get best parameters\n",
        "print(\"Best parameters:\", bayes_search.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Running Best Fit**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get predictions\n",
        "y_pred = bayes_search.best_estimator_.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"MSE: {mse:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"R²: {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMdvvfvvGfjE"
      },
      "source": [
        "# 4. Analysis and Findings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9TlD_FiAK-u"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
